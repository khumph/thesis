---
title: "Simulation: Reinforcement learning design for cancer clinical trials"
output: html_document
---

```{r}
library(pacman)
p_load(dplyr)
```

# 5.1. Simple chemotherapy mathematical model
p 11

We generate a simulated clinical reinforcement trial with N = 1000 patients (replicates)
```{r}
N <- 1000
```

with each simulated patient experiencing 6 months (T = 6) of treatment based on this ODE model. 
```{r}
Ttot <- 6
```

The initial values $W_0$ and $M_0$ for each patient are generated from independent uniform (0, 2) deviates.

$W_0$ indicates the initial value of patient's wellness
```{r}
W0 <- runif(N, min = 0, max = 2)
```

$M_0$ indicates the value of tumor size when the patient is at the beginning of the study
```{r}
M0 <- runif(N, min = 0, max = 2)
```

The treatment set consists of doses of a chemotherapy agent with an acceptable dose range of [0, 1], where the value 1 corresponds to the maximum acceptable dose. The values chosen for chemotherapy drug level $D_0$ are simulated from the uniform (0.5, 1) distribution,

```{r}
D0 <- runif(N, min = 0.5, max = 1)
```

moreover, $D_1, \ldots, D_5$ are drawn according to a uniform distribution in the interval (0, 1). 
```{r}
D <- replicate(5, runif(N, min = 0, max = 1))

D <- cbind(D0, D)
```

Thus our treatment set is restricted differently at decision time t = 0 than at other decision times to reflect a requirement that patients receive at least some drug at onset of treatment. 

Thus, the model we present must exhibit:  
(1) tumor growth in the absence of chemotherapy;  
(2) patients' negative wellness outcomes in response to chemotherapy;  
(3) the drug's capability for killing tumor cells while also increasing toxicity; and  
(4) an interaction between tumor cells and patient wellness. 

To obtain data which satisfy these requirements, we propose using a system of ordinary difference equations (ODE) modeled as follows:

$$
\dot{W}_t = a_1 (M_t \lor M_0) + b_1(D_t - d_1),
$$

```{r}
Wdot <- function(M, D, a1 = 0.1, b1 = 1.2, d1 = 0.5) { 
  a1 * M + b1 * (D - d1)
}
```

$$
\dot{M}_t = [a_2 (W_t \lor W_0) - b_2(D_t - d_2)] \times 1\{M_t > 0\},
$$

```{r}
Mdot <- function(M, W, D, a2 = 0.15, b2 = 1.2, d2 = 0.5) { 
  (a2 * W - b2 * (D - d2)) * ifelse(M > 0, 1, 0)
}
```

where time (with month as unit) $t = 0, 1, \ldots, T − 1$, and $\dot{W}_t$ and $\dot{M}_t$ indicate transition functions. Note that these changing rates yield a piecewise linear model over time. Without loss of trade-off between toxicity and efficacy, the piecewise linear model can be implemented very easily. For simplicity, we here consider tumor size instead of number of tumor cells.  
$M_t$ denotes the tumor size at time $t$.  
$W_t$ measures the negative part of wellness (toxicity).  
$D_t$ denotes the chemotherapy agent dose level. 

The indicator function term $1\{M_t > 0\}$ in (3) represents the feature that when the tumor size is absorbed at 0, the patient has been cured, and there is no future recurrence of the tumor.

The value of other different parameters for the model are fixed as: $a_1 = 0.1$, $a_2 = 0.15$, $b_1 = 1.2$, $b_2 = 1.2$, $d_1 = 0.5$ and $d_2 = 0.5$. 

```{r}
a1 <- 0.1
a2 <- 0.15
b1 <- 1.2
b2 <- 1.2
d1 <- 0.5
d2 <- 0.5
```

```{r}
W <- matrix(c(W0, numeric(6000)), ncol = 7)
M <- matrix(c(M0, numeric(6000)), ncol = 7)

for (i in 1:(Ttot)) {
  W[, i + 1] <- Wdot(M[, i], D[, i]) + W[, i]
  M[, i + 1] <- Mdot(M[, i], W[, i], D[, i]) + M[, i]
}
```


# 5.2. Q-function estimation and optimal regimen discovery

### here the hazard and rewards occur at then end of each interval/beginning of subsequent interval, so the first column is the reward/hazard/death at the end of the first interval, not the initial reward/hazard (which is zero since rewards depend on a comparison between intervals and everyone was alive initially)

We decompose this reward function $R_t$ into three parts: $R_{t, 1}(D_t, W_{t + 1}, M_{t + 1})$ due to survival status, $R_{t, 2}(W_t, D_t, W_{t + 1})$ due to wellness effects, and $R_{t, 3}(M_t, D_t, M_{t + 1})$ due to tumor size effects. It can be described by:

```{r}
R2 <- function(W1, W0) {
  ifelse(W1 - W0 <= -0.5, 5,
         ifelse(W1 - W0 >= 0.5, -5, 0))
}

R3 <- function(M1, M0) {
  ifelse(M1 == 0, 15,
         ifelse(M1 - M0 <= -0.5, 5,
                ifelse(M1 - M0 >= 0.5, -5, 0)))
}

r2 <- matrix(c(numeric(6000)), ncol = 6)
r3 <- matrix(c(numeric(6000)), ncol = 6)
for (i in 1:(Ttot)) {
  r2[, i] <- R2(W[, i + 1], W[, i])
  r3[, i] <- R3(M[, i + 1], M[, i])
}
```

In most phase III clinical trials, the primary endpoint of clinical interest is the overall survival (OS): this is why we put −60 as a high penalty for patient death. Additionally, we assign a relatively high value 15 as a bonus when a patient is cured.
We assume that survival status depends on both toxicity and tumor size. For each time interval $(t − 1, t], t = 1,\ldots, 6$, we define the hazard function as $\lambda(t)$, which satisfies

$$
\log(\lambda(t)) = \mu_0 + \mu_1 W_t + \mu_2 M_t
$$

```{r}
lambda <- function(W, M, mu0, mu1, mu2) {
  exp(mu0 + mu1 * W + mu2 * M)
}
```

where $\mu_0$, $\mu_1$, and $\mu_2$ are constant pre-specified parameters. In particular, assigning $\mu_1 = \mu_2 = 1$ indicates that we consider wellness and tumor size to have an equally weighted influence on the survival rate. 

```{r}
mu0 <- -5 ### MADE UP. NO VALUE GIVEN IN TEXT
mu1 <- 1
mu2 <- 1
```

The survival function is then

$$
\Delta F(t) = \exp[-\Delta\Lambda(t)]
$$

#### ??? What did they use for $\mu_0$?

where $\Delta\Lambda(t) = \int_{t-1}^{t}\lambda(s)ds$ is the cumulative hazard function.

$$
\begin{align*}
\Delta\Lambda(t) &= \int_{t-1}^{t}\lambda(s)ds \\
\Delta\Lambda(t) &= \int_{t-1}^{t}\exp(\mu_0 + \mu_1 W_s + \mu_2 M_s)ds \\
\Delta F(t) = \exp[- \Delta\Lambda(t)] &= \exp\left[ - \int_{t-1}^{t}\exp(\mu_0 + \mu_1 W_s + \mu_2 M_s)ds \right]
\end{align*}
$$

#### ??? In ths instance $\Delta\Lambda(t) = \int_{t-1}^{t}\lambda(s)ds = \lambda(t)$?
#### ??? Should $W_s$ and $M_s$ be treated as constants over the interval?

```{r}
lams <- matrix(numeric(6000), ncol = 6)
for (i in 1:(Ttot)) {
  lams[, i] <- lambda(W[, i + 1], M[, i + 1], mu0, mu1, mu2)
}
deltaF <- exp(-lams)
```

The reason the term $R_{t,1}(D_t, W_{t+1}, M_{t+1})$ is expressed as a function of $W_{t+1}$ and $M_{t+1}$ is that the hazard function is only determined by the states at the end of each time interval. 

#### ??? How does this follow?

The conditional probability of death for each time interval is $p = 1 − \Delta F(t)$. 
```{r}
p <- 1 - deltaF
```

The survival status (with death coded as 1) is drawn according to a Bernoulli distribution $B(p)$. 

```{r}
set.seed(1)
died <- apply(p, c(1, 2), function(p) rbinom(1, 1, p)) 
r1 <- ifelse(died == 1, -60, 0) # everytime someone dies, reward is -60
```

```{r}
# find total reward
r <- r1 + r2 + r3
```

```{r}
zeroR <- function(x, died = died, i) { 
  # zeroR function zeros rewards after death of a patient
  # inputs:
  #   matrix of total rewards (r) 
  #   matrix of deaths (died)
  #   an index of a particular patient of interest (i)
  # returns:
  #   row vector of rewards for the patient with rewards following death = 0
  if (sum(died[i, ]) >= 1) {
    # if patient died, identify first death
    first <- which(died[i, ] == 1)[1]
    # set all rewards after death to zero
    if (first < ncol(died)) {x[i, (first + 1):ncol(died)] <- 0} 
    # return new rewards with rewards after death = 0
    return(x[i, ])
  } 
  # return orginal rewards if patient did not die before end
  return(x[i, ])
}

for (i in 1:nrow(r)) {
  r[i, ] <- zeroR(r, died, i)
}
```

Overall, by letting $\gamma = 1$ (we would like to fully consider maximizing rewards in the long run), the one-step Q-learning with recursive form is utilized, with $Q_t(S_t, A_t)$ predicting

$$
\hat{R}_t = R_t + \max_{a_{t+1}} \hat{Q}_{t+1}(S_{t+1}, a_{t+1})
$$

where $R_t = R_{t, 1}(D_t, W_{t + 1}, M_{t + 1}) + R_{t, 2}(W_t, D_t, W_{t + 1}) + R_{t, 3}(M_t, D_t, M_{t + 1}), t = 0, \ldots, 5$. Note note that $\hat{r}_t$ mentioned previously is defined as a realization of $R_t$. This recursive estimation process is called SARSA (state, action, reward, next state, next action) in the reinforcement learning literature.

To obtain the estimator $\hat{Q}_t$, we apply SVR and ERT respectively for fitting $Q_t$ backward, and save the results as $\{\hat{Q}_5, \hat{Q}_4, \ldots, \hat{Q}_0\}$. 

Figure 2 illustrates the treatment plan and relevant Q-function estimation procedures. 

![](fig2.png)

```{r}
colnames(D) <- c(0:5)
colnames(M) <- c(0:6)
colnames(W) <- c(0:6)
colnames(r) <- c(0:5)
dat <- data.frame(D = D, M = M, W = W, r = r) %>% tbl_df()
```


```{r}

```

