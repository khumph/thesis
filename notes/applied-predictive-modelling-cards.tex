\documentclass[10pt]{book} 
\usepackage{amssymb,amsmath,amsfonts,mathrsfs} 
\usepackage[paperwidth=5in, paperheight=100in, textheight = 99in]{geometry} 
\pagestyle{empty}

% makes table of contents and such hyperlinks
\usepackage{hyperref}

% > < print corectly, accented words hyphenate
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc} 

% English language/hyphenation
\usepackage[english]{babel} 

% Math packages
\usepackage{amsmath, amsfonts, amsthm, amssymb} 

% lots of math mode conveniences
\usepackage{commath} 

% bold math
\usepackage{bm}

% Removes all indentation from paragraphs
\setlength{\parindent}{0pt}
% add spaces when returning
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\newcommand{\E}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\bias}{\operatorname{bias}}
\newcommand{\MSE}{\operatorname{MSE}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\h}{\hrulefill}

\begin{document}
\tableofcontents

\chapter{An Introduction to Feature Selection} % (fold)
\label{chap:an_introduction_to_feature_selection}

\section{Approaches for Reducing the Number of predictors} % (fold)
\label{sec:approaches_for_reducing_the_number_of_predictors}

What are the two main categories for reducing the number of predictors (apart from models with built-in feature selection)?

\begin{enumerate}
  \item Wrapper methods
  \item Filter methods
\end{enumerate}

\hrulefill

What are wrapper methods?

Category of approaches to doing feature selection.

Essentially search algorithms that treat predictors as inputs and use model performance as output to optimize.

(Use procedures that add and/or remove predictors to find optimal combinations that maximize model performance.)

\hrulefill

What are filter methods?

Category of approaches to doing feature selection.

Evaluate the relevance of the predictors outside of model, only include predictors in model that fulfill some criterion. 

E.g. in a classification problem, each predictor could be individually evaluated to check for a plausible relationship with observed classes.

\hrulefill

What are the strengths and weaknesses of filter methods?

Strengths:
\begin{enumerate}
  \item Usually more computationally efficient than wrapper methods
\end{enumerate}

Weaknesses:
\begin{enumerate}
  \item Selection criterion not directly related to model effectiveness
  \item Most evaluate each predictor separately, redundant variables may be retained
\end{enumerate}

\hrulefill

What are the strengths and weaknesses of wrapper methods?

Strengths of wrapper methods:
\begin{enumerate}
  \item Selection of predictors based on model performance
\end{enumerate}

Weaknesses of filter methods:
\begin{enumerate}
  \item More computationally intensive than filter methods
  \item Increased risk of overfitting
\end{enumerate}

\hrulefill

What are some examples of wrapper methods?

Stepwise selection methods: forward, backward, etc


\hrulefill

Describe the process of forward selection

Start with a model with only an intercept.

Add predictors one at a time, do a hypothesis test to see if newly added predictor is statistically significant (by some threshold). 

If at least one predictor is under the threshold, the predictor with the smallest value is added to the model.
 and process repeats again, until no predictors are below the threshold.

Linear regression is the base learner, forward selection is the search procedure. The objective function (quantity being optimized) is statistical significance (p-value)

\hrulefill

What does it mean for a procedure to be greedy?

It does not reevaluate past solutions.

\hrulefill

What is an objective function?

The quantity being optimized

\hrulefill

What are some issues with forward selection?

\begin{enumerate}
    \item Procedure is greedy
    \item Using repeated hypothesis tests on same data invalidates many of their statistical properties
    \item Maximizing statistical significance may not be the same as maximizing accuracy
\end{enumerate}

``...if this procedure had just been proposed as a statistical method, it would most likely be rejected because it violates every principal of statistical estimation and hypothesis testing.'' - Harrell


\hrulefill

How can you address the issues with forward selection?

\begin{enumerate}
    \item Procedure is greedy
    \item Using repeated hypothesis tests on same data invalidates many of their statistical properties
    \item Maximizing statistical significance may not be the same as maximizing accuracy
\end{enumerate}

First issue can be addressed by using more complex search procedures

For the second two, Use a different metric indicative of model performance (RMSE, AUROCC, AIC)


\hrulefill

What is the formula for AIC for linear regression?

\[
  AIC = n \log \del{\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^2} + 2P
\]

\hrulefill

What is AIC?

Measure of predictive ability of a model. No inferential statements made, all other things being equal model simplicity is favored over complexity.

Akaike information criterion

AIC designed for preplanned comparisons between models (not automated search)

\hrulefill

What is correlation-based feature selection?

Attempts to find the best subset of predictors that have strong correlations with the outcome but weak between-predictor correlations. 

\hrulefill



% section approaches_for_reducing_the_number_of_predictors (end)

% chapter an_introduction_to_feature_selection (end)




\end{document}