\documentclass[10pt]{article}
\usepackage{amssymb,amsmath,amsfonts,mathrsfs}
\usepackage[paperwidth=5in, paperheight=100in]{geometry}
\pagestyle{empty}
\setlength{\parindent}{0in}
\begin{document} 

\hrulefill
What is reinforcement learning?

Area of machine learning more focused on goal-directed learning from interaction than other approaches.

Basic idea: capture the most important aspects of a real problem facing a learning agent interacting with its environment to achieve a goal

Basic process involves: 

1. trying a sequence of actions, 
    - learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them out.
2. recording the consequences (reward) of those actions,
3. statistically estimating the relationship between actions and consequences,
    - If response to action is positive, it tends to increase the probability that the response will occur again in the same situation (reinforcement).
4. and then choosing the action that results in the most desirable consequence (maximize total reward)

> a computational approach to learning from interaction. Rather than directly theorizing about how people or animals learn, we explore idealized learning situations and evaluate the effectiveness of various learning methods. (the perspective of an artificial intelligence researcher or engineer)


\hrulefill

What are stochastic sequential decision processes?

reinforcement learning methods

\hrulefill

What is a reinforcement learning method?

Any method that is suited to ``solving'' Finite Markov Decision Processes we consider to be a reinforcement learning method

\hrulefill

What does it mean to ``solve'' a reinforcement learning task?

Roughly, finding a policy that achieves a lot of reward over the long run.

\hrulefill

What is Q-learning?

Doesn't estimate value function directly, estimates a Q-function instead.
requires no prior knowledge, is exploration insensitive and easy to implement, and is so far one of the most popular and seems to be the most effective model-free algorithm for learning from delayed reinforcement

once the Q functions have been estimated, it is only necessary to know the state to determine the best action.

\hrulefill

What is a Q-function?

\hrulefill

What does it mean that Q-learning is "model free"?

\hrulefill

What does it mean that Q-learning is "doesn't require prior knowledge"?

\hrulefill

What does it mean that Q-learning is "exploration insensitive"?

\hrulefill

What does it mean that Q-learning is an "off policy" TD-learning method?

\hrulefill

Why does Q-learning require less memory and less computation?

\hrulefill

How does Q-learning involve nonsmooth operations of the data?

\hrulefill

Why does nonsmoothness cause standard asymptotic approaches for inference like the bootstrap or Taylor series arguments to breakdown if applied without correction?

\hrulefill

Why is Q-learning easy to implement?

\hrulefill

Why in Q-learning is it only necessary to know the state to determine the best action, once the Q functions have been estimated.

\hrulefill

How does the Q-function optimize rewards without explicitly referencing their form??

\hrulefill

What is Temporal-difference learning (TD-learning)?

\hrulefill

Why can TD-learning find optimal polices without any knowledge of the dynamic model?

\hrulefill

What is the "eligibility traces" problem?

\hrulefill

What is an adaptive design

\hrulefill

What is a dynamic treatment regime (DTR)?

\hrulefill

What are extremely randomized trees?

\hrulefill

What are multi-stage decision problems?

\hrulefill

What is support vector regression?

\hrulefill

What is a transition graph?


\hrulefill

What is an absorbing state?

A state that transitions only to itself and generates only rewards of zero

\hrulefill

What is a backup diagram?

they diagram relationships that form the basis of the update or backup operations that are at the heart of reinforcement learning methods. These operations transfer value information back to a state (or a state– action pair) from its successor states (or state–action pairs).

unlike transition graphs, the state nodes of backup diagrams do not necessarily represent distinct states; for example, a state might be its own successor. We also omit explicit arrowheads because time always flows downward in a backup diagram


\hrulefill

What is the Bellman optimality equation?

expressed the relationship between the value of a current state and the value its following (successor) states. It does this by looking ahead and averaging over all possible future states, weighted by each state's probability of occurring. 
It states that the value of the starting state must equal the (discounted) expected value of the next state plus the reward expected along the way.

\hrulefill

Why is it not possible to directly compute an optimal policy by just solving the Bellman optimality equation (even with complete and accurate model of the environment's dynamics)?

\hrulefill

What is a value function?

functions of states (or state-action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state)

Where ``how good'' is defined in terms of future rewards that can be expected: expected return

Almost all RL algorithms involve estimating value functions

\hrulefill

What recursive relationships do value functions satisfy?


\hrulefill

What is dynamic programming

\hrulefill

What are Monte Carlo methods?

methods that average over many random samples to get a result


\hrulefill

What is the Markov property?

A state signal is Markov or has the Markov property if the state signal summarizes the past states, but retains all relevant information. 

The policy chosen using actions as a function of a Markov state is just as good as using complete histories.

\hrulefill

What is a Markov decision process (MDP)?

A reinforcement learning task the satisfies the Markov property.   

\hrulefill

What is a finite Markov decision process (finite MDP)?

A MDP, where state and action spaces are finite.

\hrulefill

What are discounted infinite horizon Markov decision processes?

\hrulefill

What is a Clinical Reinforcement trial?

new kind of clinical trial for life threatening diseases

Design consists of:
1. a finite, reasonably small set of decision times
	- could be either specific time points measured from trial onset or decision points in the treatment process such as the starting times of each new line of cancer treatment. 

2. a set of possible treatments randomize for each decision time
	- can be a continuum or a finite set
	- can include restrictions (which may be functions covariates)

3. a utility function is identified which can be assessed at each time point
	- contains an appropriately weighted combination of outcomes available at each interval between decision times and at the end of the final treatment interval.
	
may need to develop virtual patient model for design 


\hrulefill

How would one conduct a Clinical Reinforcement trial?

1. Patients recruited and randomized to the treatment set (according to protocol restrictions at each time point)

2. Outcome measures used to compute patient state and utility are obtained (for each patient at each time point)

3. repeat through end of trial

4. Q-learning applied (with SVR or ERT) at each time point (Q function can differ from time point to time point) to determine optimal treatment rule (as function of patient vars/biomarkers).

> yields individualized and time varying treatment rule superior to constant dose scheme


\hrulefill

What is a SMART design?
	
\hrulefill

What are transition functions?

A function from (state/input symbol) to state describing what state to move to on receiving a given input in a given state.

\end{document}