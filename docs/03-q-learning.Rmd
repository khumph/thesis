# Q-learning

Following the notation in @dtr-book, suppose patients are recruited into a clinical trial and covariates (states) are measured before the trial begins and recorded in a vector, \(O_{1}\). These covariates will be referred to as the patient history at stage 1: \(H_{1} \equiv O_{1}\), which may include the baseline level of the outcome. A first treatment, \(A_{1}\), is randomly assigned to each patient and each patient is measured again for the same covariates as before, the new values of which are recorded in \(O_{2}\), and a reward is received, \(R_{2}\). As mentioned above, this reward can be any scalar, which if maximized, would achieve the goals of the study. Most often rewards are simply set to be a continuous outcome of interest (e.g. kg of weight lost in a weight loss study). The treatment and second observation of covariates (which may include the first stage outcome and/or rewards) are incorporated into each patients' history at stage 2: \(H_{2} \equiv (O_{1}, A_{1}, O_{2})\).

A second treatment, \(A_{2}\) is then assigned to each patient at random (potentially depending on \(H_{2}\)) and is followed by another outcome/reward, \(R_{3}\). This process can be repeated through as many stages as desired. Note that this scheme can easily be modified for a single terminal outcome by setting \(R_{2}, \ldots, R_{K} = 0\) for a sequence of \(K\) stages (as we do in the simulation below).

The basic process described above is a Markov decision process. Markov decision processes are Markov chains with the addition of actions (adding choices) and rewards (motivation/goals). In a Markov decision process the agent and environment interact over a series of stages. At a stage \(k\), the agent is presented with a state \(S_k \in \mathcal{S}\) from the set of possible states, and chooses an action \(A_{k} \in \mathcal{A}(S_{k})\) from the set of possible actions available in that state. At the next time step, the agent receives a numerical reward, \(R_{k+1}\) as a consequence of the action chosen and is presented with a new state \(S_{k+1}\). The states in a Markov decision process have the Markov property, which means that they summarize all the information from previous steps without a reduction in what's relevant to make the best action decision. Reinforcement learning methods try to determine how the agent should change its policy in response to experience (available data) in order to maximize reward. Hence ``solving'' a reinforcement learning problem means finding a policy that achieves the most reward over the long run.

The expected return (the sum of rewards) for an agent starting in state \(s\), taking action \(a\), and then following policy \(\pi\) thereafter, called the action-value function for policy \(\pi\) (and in the context of Q-learning, a Q-function), is denoted:

\begin{equation}
  Q^{\pi}_{k}(s,a) \doteq
    \operatorname{E}_{\pi}\left[
      \sum_{j=k}^{K} R_{j+1} \mid S_{k} = s, A_{k} = a
    \right]
\end{equation}

All action-value (Q) functions satisfy a recursive relationship called the Bellman equation for \(Q_{k}^{\pi}\):

\begin{align}
  Q_{k}^{\pi}(s,a) &=
  \operatorname{E}_{\pi}\left[
      \sum_{j=k}^{K} R_{j+1} \mid S_{k} = s, A_{k} = a\right] \nonumber\\
     &=
    \operatorname{E}_{\pi}\left[
     R_{k+1}+ \sum_{j=k+1}^{K} R_{j+1} \mid S_{k} = s, A_{k} = a \right] \nonumber\\
     &=
     \operatorname{E}_{\pi}\left[R_{k+1}\mid S_{k} = s, A_{k} = a\right]+  \sum_{s'}\left[ \Pr(S_{k+1} = s'|S_{k} = s, \pi(S_{k} = s))\sum_{j=k+1}^{K} R_{j+1}\right]\nonumber\\
     &=
     \operatorname{E}_{\pi}\left[R_{k+1}\mid S_{k} = s, A_{k} = a\right]+  \sum_{s'}\left[ \Pr(S_{k+1} = s'|S_{k} = s, \pi(S_{k} = s))V_{k+1}^{\pi}(s')\right]\nonumber\\
     \mbox{or}\nonumber\\
 Q_{k}^{\pi}(s,a)    &=
     \operatorname{E}_{s'\sim\Pr(s'|s,a)}\left[R_{k+1}+   V_{k+1}^{\pi}(s') \right]\nonumber\\
     &=
       \operatorname{E}_{s'\sim\Pr(s'|s,a)}\left\{ R_{k+1}+\operatorname{E}_{a'\sim \pi} [Q_{k+1}^{\pi}(s',a')] \right\}, \\
\text{where } \nonumber \\
  V_{k}^{\pi}(s) &\doteq
    \operatorname{E}_{\pi}\left[
      \sum_{j=k}^{K} R_{j+1} \mid S_{k} = s
    \right] \\
    &= \operatorname{E}_{a\sim\pi} \left[ Q_{k}^{\pi}(s,a)  \right].
\end{align}

With the Q-function for stage \(k\) in hand, one can obtain the optimal Q-function, which is the action-value function obtained when following the optimal policy  \(\pi^{*}\), the policy that leads to the maximum cumulative rewards. This is done simply by finding the action \(a\) that maximizes \(Q^{\pi}_{k}(s,a)\) for each stage \(k\):

\begin{equation}
   Q^{*}_{k}(s, a) = \max_{a} Q^{\pi}_{k}(s, a)
\end{equation}
The optimal action at stage \( k \) is then the action which maximizes the optimal Q-function
\begin{equation}
  a^{*}_{k} = \operatorname{argmax}_{a} Q^{*}_{k}(s, a)
\end{equation}
It turns out that there is always at least one optimal policy for every Markov decision process [@Sutton2016].

Optimal Q-functions satisfy a recursive relationship called the Bellman optimality equation for \(Q^{*}\), which describes the relationship between adjacent stage optimal Q-functions:

\begin{align}
  Q_{k}^{*}(s,a) &= \operatorname{E}_{s'\sim\Pr(s'|s,a)}\left\{ R_{k+1}+\max_{a'} [Q_{k+1}^{*}(s',a')] \right\} 
\end{align}

With the Bellman optimality equation, optimal Q-functions, and the state-transition probabilities, \(\Pr(S_{k+1} = s' \mid S_k = s, A_k = a)\), we can use value iteration from dynamic programming to iterate through the stages backwards from the last stage to the first stage, obtaining at each stage the optimal Q-function and hence the optimal action. The sequence of optimal actions obtained is an optimal policy \(\pi^{*}\). In reinforcement learning problems, however, this dynamic programming approach equation won't work because the state-transition probabilities are unknown.

Q-learning overcomes this limitation by directly estimating the optimal Q-functions, for each stage, starting with the last stage. Dynamic programming is then used iterating backwards through the stages, obtaining estimates of each Q-function, \(\hat{Q}^{*}_{k}(s, a)\), at each step. Each \(\hat{Q}^{*}_{k}(s, a)\) can then be used to estimate optimal actions, \(\hat{A}^{*}\) (shown below) for each stage \(k\). Since Q-learning does not require a model of the state-transition probabilities it is described as being ``model-free''.
% The algorithm goes backwards because this avoids simply choosing the greedy move at each stage, instead taking long term consequences into account.

The estimated optimal Q-functions from Q-learning have been shown to converge to the true optimal action-value functions with probability one for finite Markov decision processes (MDPs with a finite number of states and actions), given that actions are repeatedly sampled in all states [@Watkins1992].

When there are many states and/or actions (as there are in most problems) \(Q^{*}_{k}(s, a)\) can be approximated using functions (as we do below), the parameters of which are adjusted to better match new observations over time. ^[Unfortunately these modifications mean that convergence can no longer be guaranteed in all cases [@Szepesvari2010]].


## Estimating Q-functions and optimal DTRs

In our context, the full recursive form of Q-learning can be written as:

\begin{equation}
   \hat{Q}^{\pi}_{k}(A_{k}, H_{k}) = \operatorname{E}_{\pi}[r_{k+1} + \max_{a_{k+1}} \hat{Q}^{\pi}_{k+1}(A_{k+1}, H_{k+1}) \mid A_{k}, H_{k}], \quad k = 1, ..., K
\end{equation}

The optimal treatment for each participant \(i \in 1, ..., n\) at each stage \(k\) is the treatment which maximizes the corresponding stage's Q-function:

\begin{equation}
  \hat{A}^{*}_{ik} = \operatorname{argmax}_{a_{ik}} \hat{Q}^{\pi}_{k}(A_{ik}, H_{ik}), \enspace i = 1, ..., n; \enspace k = 1, \ldots, K
\end{equation}

Since Q-functions are conditional expectations, standard regression techniques are applicable and hence the most popular method for estimating Q-functions in the health sciences is ordinary least squares [@dtr-review].

That being said, any modeling method can be used, and as mentioned above, @crt have used extremely randomized trees and support vector regression to fit Q-functions. Note that Q-functions differ across stages of treatment. The steps of Q-learning are as follows:

1. Set the Q-function after the final stage to be 0, \(\hat{Q}^{\pi}_{K+1} \equiv 0\) (though any value will do).

1. Create a pseudo-outcome for the last stage, \(K\), by adding the rewards actually received from that stage to the estimated rewards that would have been obtained if the optimal dynamic treatment regime was followed after stage \(K\):
\begin{align}
  \hat{r}_{K+1} &= r_{K+1} + \max_{a_{K+1}} \hat{Q}^{\pi}_{K+1}(A_{K+1}, H_{K+1}) \\
    \hat{r}_{K+1} &= r_{K+1}
\end{align}

1. Estimate the preceding stage Q-function, \(Q^{\pi}_{K - 1}\), using the pseudo-outcome just created, \(\hat{r}_{K}\), as the outcome with all aspects of patient history at that stage, \(H_{K}\), desired as covariates, in addition to (if fitting a linear regression model) treatment by covariate interactions for covariates in \(H_{K}\) thought to be indicative of subgroups with differential treatment effects:
\begin{equation}
      \hat{Q}^{\pi}_{K}(A_{K}, H_{K}) = \operatorname{E}[\hat{r}_{K+1} \mid A_{K}, H_{K}]
\end{equation}

1. Create the pseudo-outcome for the previous stage \(K - 1\), again as if the optimal treatment regime will be followed after \(K - 1\):
\begin{equation}
    \hat{r}_{K+1} = r_{K+1} + \max_{a_K} \hat{Q}^{\pi}_{K}(A_{K}, H_{K})
\end{equation}

1. Repeat until you've estimated the Q-function for the first stage

<!-- backwards induction -->

It is important to note that in practice, the process described above should be repeated through a process called policy iteration. New patients would be recruited and the estimated optimal dynamic treatment regime estimated from the previous trial would be used to assign treatments to each patient. Then, an updated estimate of the optimal treatment regime would be obtained by using Q-learning on these new observations. This updated optimal dynamic treatment regime would then be used to assign treatments to a new group of patients, and the process would be repeated again. After each new trial, the updated optimal dynamic treatment regime gets closer to the true optimal dynamic treatment.
