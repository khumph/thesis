# Introduction {#intro}


## Personalized medicine

<!-- % Why personalized medicine? -->

Patients often show significant differences in their responses to treatment. Adverse reactions to drugs alone contribute to significant public health burden despite these drugs being shown to be beneficial (on average) in clinical trials [@Pirmohamed2004]. Not only are there subpopulations for whom treatments are especially deleterious, some groups have a more robust response to certain treatments. Significant gains in public health can therefore be made from identifying these subgroups and the corresponding treatments which are most effective for them, rather than relying on identification of only the most effective treatment overall.

<!-- % What is personalized medicine? -->

This is the idea of *personalized medicine*^[Personalized medicine is also called precision medicine, stratified medicine, and P4 medicine], which is the commonsensical notion that the best treatment for a given patient depends upon that patients' characteristics [@pm-defn]. The term is most often used today when the patient characteristics are biomarkers, perhaps whether metastatic breast cancer cells  over-express human epidermal growth factor receptor 2 (are HER2-positive), and these biomarkers are used to determine the best treatment for a particular individual, and when to use it. In the case of the patients with HER2-positive cells, they should receive a different treatment (trastuzumab) from those with HER2-negative cells [@Baselga2006].
<!-- % When/where did personalized medicine start? -->
However, the general idea goes back at least as far as Hippocrates^[Hippocrates is attributed with saying "It is more important to know what sort of person has a disease than to know what sort of disease a person has." [@Fischer2015]] and is implemented in principle whenever a physician tries to inform a treatment decision by determining whether the cause of an infection is bacterial or viral.

While personalized medicine in principle has a long history and shows considerable promise, there are numerous challenges in identifying these patient subgroups and the optimal treatment for each. Chief among them is the question of how to select from what is often a multitude of plausible subgroups. Narrowing down these subgroups is hindered by disease processes which are usually quite complicated and not well understood. Furthermore, optimal treatments can change over time with changing disease. This is especially true of chronic diseases, however little evidence is available to inform how to tailor treatments over time and using the information from previous treatments.

## Statistical methods to personalize medicine

### Traditional methods

<!-- % traditional methods: ad hoc subgroup analyses, or searching for treatment by subgroups interactions -->

Statistically, the personalization of medicine involves the identification of patient subgroups which respond to treatment differently. Traditionally, this is accomplished by investigation of (often ad hoc) treatment by covariate interactions, where different values of a covariate identify subgroups ([@Byar1985]). Using the HER2 breast cancer example above, we would encapsulate our knowledge of a patient's HER2 status into a binary covariate which equaled one if her tumor was HER2 postitive or zero if her tumor was HER2 negative. We would then statistically test if the coefficient on an interaction term with this indicator and treatment is significantly different from zero.

<!-- % Problems: Curse of dimensionality, multiple comparisons -->

While straightforward, this approach has several problems. First, which subgroups to examine is often a subjective judgement made by the researcher (as is which cutoff to use to create subgroups from continuous covariates). Second, since the number of plausible subgroups can be quite large in typical problems, the number of interaction terms required must also be large. In addition to issues with multiple comparison, this requires much larger sample sizes (i.e. it suffers from the curse of dimensionality).

### Algorithms for detecting interactions

<!-- % interaction trees - easy to interpret, final trees don't connect with any objective function - hard to determine optimal treatment for patients -->

<!-- % Su, X., Tsai, C.-L., Wang, H., Nickerson, D. M., and Li, B. (2009), “Subgroup analysis via recursive partitioning,” Journal of Machine Learning Research, 10, 141–158. -->
<!-- % introduces interaction trees - applys to wage data -->

<!-- % Lipkovich, I., Dmitrienko, A., Denne, J., and Enas, G. (2011), “Subgroup identification based on differential effect search A recursive partitioning method for establishing response to treatment in patient subpopulations,” Statistics in Medicine, 30, 2601–2621. -->
<!-- % method of splitting to maximize treatment effect in one daughter node relative to other -->
<!-- % SIDES paper - has examples of personalized medicine in intro -->

<!-- % STIMA, others could also be included in this group -->

In response to the above problems with the traditional approach, several novel algorithms to detect interactions have been developed. Many use recursive partitioning (the basic process of which is described below) with a splitting criterion based on the degree of interaction (@Zeileis2008; @Su2009; @Lipkovich2011; @Dusseldorp2014). These (and models based on recursive partitioning in general) have the advantage of being easy to interpret.

<!-- two-step methods: first step: estimate differential treatment effect of each ind by score function, then use scores as response in second step - (Cai et al., 2011; Zhao et al., 2013; Foster et al., 2011; Faries et al., 2013) \\ -->
<!-- need to impose parametric models that may be misspecified, hard to interpret -->
<!-- Cai, T., Tian, L., Wong, P. H., and Wei, L. (2011), “Analysis of randomized comparative clinical trial data for personalized treatment selections,” Biostatistics, 12, 270–282. -->
<!-- Zhao, L., Tian, L., Cai, T., Claggett, B., and Wei, L.-J. (2013), “Effectively selecting a target population for a future comparative study,” Journal of the American Statistical Association, 108, 527–539. -->
<!-- Foster, J. C., Taylor, J. M., and Ruberg, S. J. (2011), “Subgroup identification from randomized clinical trial data,” Statistics in Medicine, 30, 2867–2880. -->
<!-- Faries, D. E., Chen, Y., Lipkovich, I., Zagar, A., Liu, X., and Obenchain, R. L. (2013), “Local control for identifying subgroups of interest in observational research: Persistence of treatment for major depressive disorder,” International Journal of Methods in Psychiatric Research, 22, 185–194. -->

Other methods use a two-step process (@Cai2011; @Zhao2013; @Foster2011). The first step is to estimate the differential treatment effect of each participant and use this to develop a score to group patients. These scores are then used as a response in the second step to estimate the mean treatment difference between groups. These methods require a parametric or semi-parametric model in the first step which is subject to misspecification.

<!-- Maximize value function -->
<!-- Qian, M. and Murphy, S. A. (2011), “Performance guarantees for individualized treatment rules,” Annals of Statistics, 39, 1180. -->
<!-- Zhao, Y., Zeng, D., Rush, A. J., and Kosorok, M. R. (2012), “Estimating individualized treatment rules using outcome weighted learning,” Journal of the American Statistical Association, 107, 1106–1118. -->
<!-- Zhang, B., Tsiatis, A. A., Davidian, M., Zhang, M., and Laber, E. (2012), “Estimating optimal treatment regimes from a classification perspective,” Stat, 1, 103–114. -->

A third class of methods maximize mean responses across patients and subgroups either by using penalized regression [@Qian2011], or by treating the identification of subgroups as a classification problem with each participant weighted by their outcome (@Zhao2012; @Zhang2012).

These and other methods are promising but none address how to select the optimal value of a continuous treatment, so it is not clear how to extend them to address our goal of determining optimal dosing strategies. Perhaps more importantly, none of the methods described above directly consider more than a single stage of treatment. In practice treatments are often changed over time based off of patient response and other factors, particularly in the management of chronic disease. To truly personalize medicine, therefore, it is important to determine how to adapt treatment over time to changing situations.

<!-- % All limited to single step? -->

<!-- % How can reinforcement learning be of use in determining personalized treatments? -->

## Overview of contents

In this paper, we will see how the obstacles to identifying optimal treatment regimes for a given individual can be addressed using Q-learning, a algorithm from reinforcement learning, in conjunction with statistical learning models. We will see how these methods can dramatically increase survival times in a simulated cancer clinical trial where doses of a single drug are chosen at each month of treatment, without a model of the disease process or a priori hypotheses about subgroup membership. The simulation was inspired by @crt who achieved similar results using extremely randomized trees and support vector regression to estimate Q-functions. We will replicate and extend their results to more scenarios (e.g. situations with many meaningless covariates and patient subgroups), contrast their results with results obtained using more interpretable models, and compare all of the above to the best possible treatment sequence chosen with complete knowledge of disease processes and their effects on survival.

We will first briefly overview the basic elements of reinforcement learning, reinforcement learning's relevance for personalized medicine, and Q-learning [@Watkins1989] specifically. We will then discuss the supervised learning methods that I used in conjunction with Q-learning to estimate optimal dosing strategies: regression tress constructed using the classification and regression trees (CART) method [@CART], Multivariate adaptive regression splines (MARS -- @mars), and random forests (RF -- @rf), with random forests serving as a less interpretable comparison, similar to the extremely randomized trees used by @crt. Each technique will be illustrated with a simple example taken from the simulation. We will then discuss the details of the simulation setup and its results.

For our purposes, trees (and MARS, which shares many of their desirable properties) have several particularly appealing features. First, the tree and MARS model building procedures conduct covariate selection as part of the model building process. This will help us screen non-informative variables. Second, we need not specify the form of the relationship between the covariates and the outcome, or limit ourselves to linear or additive relationships. This will allow us to choose optimal actions without prior knowledge of disease mechanisms, which are often complicated and not well understood. Third, since single trees and MARS models are quite interpretable, we can in principle inspect them to see which variables interact with treatment and thus identify patient subgroups.

