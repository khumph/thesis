---
title: 'Simulation: Reinforcement learning design for cancer clinical trials'
output: html_document
---

# 5.1. Simple chemotherapy mathematical model
p 11

We generate a simulated clinical reinforcement trial with N = 1000 patients (replicates) with each simulated patient experiencing 6 months (T = 6) of treatment based on this ODE model. 

The initial values $W_0$ and $M_0$ for each patient are generated from independent uniform (0, 2) deviates.

$W_0$ indicates the initial value of patient's wellness  
$M_0$ indicates the value of tumor size when the patient is at the beginning of the study

ahe treatment set consists of doses of a chemotherapy agent with an acceptable dose range of [0, 1], where the value 1 corresponds to the maximum acceptable dose.  

The values chosen for chemotherapy drug level $D_0$ are simulated from the uniform (0.5, 1) distribution,   
moreover, $D_1, \ldots, D_5$ are drawn according to a uniform distribution in the interval (0, 1). 

Thus our treatment set is restricted differently at decision time t = 0 than at other decision times to reflect a requirement that patients receive at least some drug at onset of treatment. 

Thus, the model we present must exhibit:  
(1) tumor growth in the absence of chemotherapy;  
(2) patients' negative wellness outcomes in response to chemotherapy;  
(3) the drug's capability for killing tumor cells while also increasing toxicity; and  
(4) an interaction between tumor cells and patient wellness. 

To obtain data which satisfy these requirements, we propose using a system of ordinary difference equations (ODE) modeled as follows:

$$
\dot{W}_t = a_1 (M_t \lor M_0) + b_1(D_t - d_1)
$$

$$
\dot{M}_t = [a_2 (W_t \lor W_0) - b_2(D_t - d_2)] \times 1\{M_t > 0\},
$$

where time (with month as unit) $t = 0, 1, \ldots, T − 1$, and $\dot{W}_t$ and $\dot{M}_t$ indicate transition functions. Note that these changing rates yield a piecewise linear model over time. Without loss of trade-off between toxicity and efficacy, the piecewise linear model can be implemented very easily. For simplicity, we here consider tumor size instead of number of tumor cells.  
$M_t$ denotes the tumor size at time $t$.  
$W_t$ measures the negative part of wellness (toxicity).  
$D_t$ denotes the chemotherapy agent dose level. 

The indicator function term $1\{M_t > 0\}$ in (3) represents the feature that when the tumor size is absorbed at 0, the patient has been cured, and there is no future recurrence of the tumor.

The value of other different parameters for the model are fixed as: $a_1 = 0.1$, $a_2 = 0.15$, $b_1 = 1.2$, $b_2 = 1.2$, $d_1 = 0.5$ and $d_2 = 0.5$. 

# 5.2. Q-function estimation and optimal regimen discovery

**here the hazard and rewards occur at then end of each interval/beginning of subsequent interval, so the first column is the reward/hazard/death at the end of the first interval, not the initial reward/hazard (which is zero since rewards depend on a comparison between intervals and everyone was alive initially)**

We decompose this reward function $R_t$ into three parts: $R_{t, 1}(D_t, W_{t + 1}, M_{t + 1})$ due to survival status, $R_{t, 2}(W_t, D_t, W_{t + 1})$ due to wellness effects, and $R_{t, 3}(M_t, D_t, M_{t + 1})$ due to tumor size effects. It can be described by:

In most phase III clinical trials, the primary endpoint of clinical interest is the overall survival (OS): this is why we put −60 as a high penalty for patient death. Additionally, we assign a relatively high value 15 as a bonus when a patient is cured.
We assume that survival status depends on both toxicity and tumor size. For each time interval $(t − 1, t], t = 1,\ldots, 6$, we define the hazard function as $\lambda(t)$, which satisfies

$$
\log(\lambda(t)) = \mu_0 + \mu_1 W_t + \mu_2 M_t
$$

where $\mu_0$, $\mu_1$, and $\mu_2$ are constant pre-specified parameters. In particular, assigning $\mu_1 = \mu_2 = 1$ indicates that we consider wellness and tumor size to have an equally weighted influence on the survival rate. 

The survival function is then

$$
\Delta F(t) = \exp[-\Delta\Lambda(t)]
$$

where $\Delta\Lambda(t) = \int_{t-1}^{t}\lambda(s)ds$ is the cumulative hazard function.

$$
\begin{align*}
\Delta\Lambda(t) &= \int_{t-1}^{t}\lambda(s)ds \\
\Delta\Lambda(t) &= \int_{t-1}^{t}\exp(\mu_0 + \mu_1 W_s + \mu_2 M_s)ds \\
\Delta F(t) = \exp[- \Delta\Lambda(t)] &= \exp\left[ - \int_{t-1}^{t}\exp(\mu_0 + \mu_1 W_s + \mu_2 M_s)ds \right]
\end{align*}
$$

The reason the term $R_{t,1}(D_t, W_{t+1}, M_{t+1})$ is expressed as a function of $W_{t+1}$ and $M_{t+1}$ is that the hazard function is only determined by the states at the end of each time interval. 

The conditional probability of death for each time interval is $p = 1 − \Delta F(t)$. 

The survival status (with death coded as 1) is drawn according to a Bernoulli distribution $B(p)$. 

Overall, by letting $\gamma = 1$ (we would like to fully consider maximizing rewards in the long run), the one-step Q-learning with recursive form is utilized, with $Q_t(S_t, A_t)$ predicting

$$
\hat{R}_t = R_t + \max_{a_{t+1}} \hat{Q}_{t+1}(S_{t+1}, a_{t+1})
$$

where $R_t = R_{t, 1}(D_t, W_{t + 1}, M_{t + 1}) + R_{t, 2}(W_t, D_t, W_{t + 1}) + R_{t, 3}(M_t, D_t, M_{t + 1}), t = 0, \ldots, 5$. Note note that $\hat{r}_t$ mentioned previously is defined as a realization of $R_t$. This recursive estimation process is called SARSA (state, action, reward, next state, next action) in the reinforcement learning literature.

To obtain the estimator $\hat{Q}_t$, we apply SVR and ERT respectively for fitting $Q_t$ backward, and save the results as $\{\hat{Q}_5, \hat{Q}_4, \ldots, \hat{Q}_0\}$. 

Figure 2 illustrates the treatment plan and relevant Q-function estimation procedures. 

![](fig2.png)


```{r}
rm(list = ls())
library(pacman)
p_load(tidyverse, gridExtra, rpart, rms, ranger, earth, stringr)
```

```{r sim}
source("sim.R")
source("qlearn-functs.R")
set.seed(20161116)
dat_long <- sim()
```

```{r, eval=FALSE, include=FALSE}
dat <- dat_long %>% filter(month == 5 | month == 6)

dat <- dat %>% mutate(reward = lag(reward))

dat_long %>%
    select(ID,
           month,
           dose,
           reward,
           tumor_mass,
           toxicity,
           died) %>%
    mutate(reward = lag(reward))

dat %>% group_by(ID) %>%  mutate(
  r = R2(lead(toxicity), toxicity, reward.type = "orig") +
      R3(lead(tumor_mass), tumor_mass, reward.type = "orig") +
          ifelse(lead(died, default = 0) == 1, -60, 0)
) %>% ungroup() %>% summarise(mean(r, na.rm = T))

dat %>% filter(tumor_mass > 0.5, dose < 0.5 | is.na(dose)) %>% group_by(ID) %>%  mutate(
  r = R2(lead(toxicity), toxicity, reward.type = "orig") +
      R3(lead(tumor_mass), tumor_mass, reward.type = "orig") +
          ifelse(lead(died, default = 0) == 1, -60, 0)
) %>% ungroup() %>% summarise(mean(r, na.rm = T))

dat %>% filter(tumor_mass > 0.5, dose > 0.5 | is.na(dose)) %>% group_by(ID) %>%  mutate(
  r = R2(lead(toxicity), toxicity, reward.type = "orig") +
      R3(lead(tumor_mass), tumor_mass, reward.type = "orig") +
          ifelse(lead(died, default = 0) == 1, -60, 0)
) %>% ungroup() %>% summarise(mean(r, na.rm = T))

dat %>% filter(tumor_mass < 0.5, dose < 0.5 | is.na(dose)) %>% group_by(ID) %>% mutate(
  r = R2(toxicity, lag(toxicity), reward.type = "orig") +
      R3(tumor_mass, lag(toxicity), reward.type = "orig") +
          ifelse(died == 1, -60, 0)
) %>% ungroup() %>% summarise(mean(r, na.rm = T))

dat %>% filter(tumor_mass < 0.5, dose > 0.5 | is.na(dose)) %>% group_by(ID) %>% mutate(
  r = R2(toxicity, lag(toxicity), reward.type = "orig") +
      R3(tumor_mass, lag(toxicity), reward.type = "orig") +
          ifelse(died == 1, -60, 0)
) %>% ungroup() %>% summarise(mean(r, na.rm = T))
```

```{r}
tox_mass_plot <- ggplot(
  data = filter(dat_long, ID == 66)
) +
  geom_line(
    mapping = aes(x = month, y = toxicity, group = ID), color = "green"
  ) +
  geom_line(
    mapping = aes(x = month, y = tumor_mass, group = ID) 
  )

dose_plot <- ggplot(
  data = filter(dat_long, ID == 60)
) +
  geom_line(
    mapping = aes(x = month, y = dose, group = ID), color = "red"
  ) + ylim(0, 1)

grid.arrange(tox_mass_plot, dose_plot, nrow = 2, ncol = 1)
```


# Q-learning

1. Inputs: a set of training data consists of attributes $x$ (states $s_{t}$, actions $a_t$) and index $y$ (rewards $r_{t}$), i.e.$\{(s_{t}, a_{t}, r_{t})_{i}; \; t = 0, \ldots, T; \; i = 1, \ldots, N\}$.
2. Initialization: Let $t = T + 1$ and $\hat{Q}T+1$ be a function equal to zero on $S_{t} \times A_{t}$.

3. Iterations: repeat computations until stopping conditions are reached $(t = 0)$.
    a. t <- t − 1 .
    b. $Q_t$ is fitted with support vector regression (SVR) or extremely randomized trees (ERT) through the following recursive equation:
$$
Q_{t}(s_{t}, a_{t}) = r_{t} + max_{a_{t+1}}\hat{Q}_{t+1}(s_{t+1}, a_{t+1}) + \epsilon
$$

c. Use cross-validation to choose tuning parameters C and $\zeta$ if fitting Qt via SVR with Gaussian kernel; choose plausible values of parameters K, G, nmin if fitting Qt via ERT (K = 3,G = 50, nmin = 2 in our simulation).
}
4. Given the sequential estimates of \{Q̂0, Q̂1,. .,5\}, the sequential individualized optimal polices {π̂0,..., π̂5} for application to the virtual phase III trial are computed.



### Initialize $Q(s, a)$, for all $s \in \mathcal{S}$, $a \in \mathcal{A}(s)$, arbitrarily, and $Q(\texttt{terminal_state}, \cdot) = 0$ 

### Repeat (for each episode):
Initialize $S$

### Repeat (for each step of episode):
1. Choose $A$ from $S$ using policy derived from $Q$ (e.g., epsilon-greedy) Take action $A$, observe $R, S'$

(done in simulation by random assignment)

2. 
$$
Q(S, A) \leftarrow Q(S, A) + \alpha * (R + \gamma * \max_a Q(S',a) - Q(S, A))
$$

3. $S \leftarrow S'$

### Until S is terminal

```{r qlearn}
set.seed(20161027)
dat_long <- dat_long %>% mutate(
  Q_hat = ifelse(month == 6, NA,
                 ifelse(month == 5, reward, 0)),
  best = ifelse(month == 6, NA, 999)
)

Q <- Qlearn(
  data = dat_long,
  formula = Q_hat ~ tumor_mass + dose + toxicity,
  treatment = "dose",
  method = "rcs" #,
  # degree = 3
)
```


```{r}
dat <- align_df(Q)

ggplot(filter(dat, month == 5)) +
  geom_point(aes(x = tumor_mass, y = reward)) +
  geom_point(aes(x = tumor_mass, y = Q_hat), color = "blue")
```


```{r xplor-plots, eval=FALSE, include=FALSE}


mod <- Q$mod_list[[5]]

ggplot(dat, aes(x = predict(mod), y = residuals(mod))) + geom_point()

ggplot(dat) +
  geom_point(aes(x = tumor_mass, y = reward)) +
  geom_point(aes(
    x = tumor_mass,
    y = predict(mod)
  ), color = "blue")

ggplot(dat) +
  geom_point(aes(x = toxicity, y = reward)) +
  geom_point(aes(x = toxicity, y = predict(mod)), color = "blue")
```

```{r}
dat <- filter(dat, month == 5)
nested_df <- max_df(dat, Q$mod_list[[4]], Q$formula, idvar = "ID", method = "rcs", nested = T)

set.seed(8)
i <- sample(nested_df$ID, 1)
ggplot(filter(nested_df, ID == i), aes(x = dose, y = preds)) +
  geom_point()

set.seed(77)
ggplot(data = filter(nested_df, ID %in% sample(1:1000, 50))) +
  geom_line(mapping = aes(x = dose, y = preds, group = ID))
```

```{r}
set.seed(20170128)
sim_test(Q) %>% plots_tab()
```

