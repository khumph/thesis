\documentclass[12pt]{article}
\usepackage{geometry}

% makes table of contents and such hyperlinks
\usepackage{hyperref}

% nicely formatted tables
\usepackage{booktabs}

% H to force figures and tables to where they appear in text
\usepackage{float}

% bold math symbols (including greek)
\usepackage{bm}

% English language/hyphenation
\usepackage[english]{babel}

% > < print corectly, accented words hyphenate
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% allows usage of \enquote to quote things
\usepackage{csquotes}

% \usepackage[backend = bibtex, sorting=none, hyperref = true]{biblatex}
% \addbibresource{spray.bib}
\usepackage{cite}
\usepackage{ctable}

\usepackage{amsmath}
\usepackage{commath}
\usepackage[]{algorithm2e}

\DeclareMathOperator*{\argmax}{argmax}
\RestyleAlgo{boxruled}

%--------------------------------------------------------------------------
%  TITLE SECTION
%--------------------------------------------------------------------------

\title{\normalfont \Large Using reinforcement learning to personalize dosing strategies in a simulated cancer trial with high dimensional data}

\author{\normalsize \sl Kyle Humphrey}

\date{\normalsize \sl \today}

\begin{document}

\maketitle

\tableofcontents
\listoffigures
\listoftables

\section{Introduction}

\subsection{Personalized medicine} % (fold)
\label{sub:personalized_medicine}

% Why personalized medicine?}

Patients often show significant differences in their responses to treatment. [\textbf{provide some examples}] Significant gains in public health can be made from identifying subgroups and the corresponding treatments which are most effective for them, rather than relying on identification of only the most effective treatment overall.

% What is personalized medicine?

This is the idea of \emph{personalized medicine}\footnote{Personalized medicine is also called precision medicine, stratified medicine, and P4 medicine}, which is the commonsensical notion that the best treatment for a given patient depends upon that patients' characteristics \cite{defn-paper}. The term is most often used today to when the characteristics are biomarkers, perhaps whether metastatic breast cancer cells  overexpress human epidermal growth factor receptor 2 (HER2), and these biomarkers are used to determine the best treatment for a particular individual, and when to use it. In the case of the patients with HER2-positive cells, they should receive a different treatment (trastuzumab) from those with HER2-negative cells \cite{} .
% When/where did personalized medicine start?
However, the general idea goes back at least as far as Hippocrates \textbf{[cite]}, and are implemented in principle whenever a physician tries to inform a treatment decision by determining whether the cause of an infection is bacterial or viral.

% subsection personalized_medicine (end) 


% What are some challenges to implementing/discovering personalized treatments?}

Lots of possible subgroups, complexity of disease

% What are some methods that have been used in the past to find personalized treatment regimes?
\subsection{Statistical methods to personalize medicine} % (fold)
\label{sub:statistical_methods_to_personalize_medicine}

% subsection statistical_methods_to_personalize_medicine (end)

\subsubsection{Traditional methods} % (fold)
\label{ssub:traditional_methods}

% traditional methods: ad hoc subgroup analyses, or searching for treatment by subgroups interactions 

Statistically, the personalization of medicine involves the identification of patient subgroups which respond to treatment differently. Traditionally, this is accomplished by investigation of (often ad hoc) treatment by covariate interactions, where values of the covariate identify subgroups. Using the HER2 breast cancer example above, we could encapsulate our knowledge of a patient's HER2 status into a binary covariate which indicates whether she was HER2 positive or negative, then statistically test if the coefficient on an interaction term between the indicator of HER2 status and treatment is significantly different from 0.

% Problems: Curse of dimensionality, multiple comparisons

While straightforward, this approach has several problems. First, which subgroups to examine is often a subjective judgement made by the researcher (and where to cutoff a group that is formed based on a continuous covariate). Second, since the numer of plausible subgroups can grow large quickly the number of interaction terms required can grow quite large in typical problems causing a reduction in power and requiring an much greater growth of sample size (curse of dimensionality). There are also issues to be addressed with multiple comparisons.

\subsubsection{Algorithms for detecting interactions} % (fold)
\label{ssub:algorithms_for_detecting_interactions}

In response to the above problem with the traditional approach, several novel algorithms to detect interactions have been developed.

interaction trees - easy to interpret, final trees don't connect with any objective function - hard to determine optimal treatment for patients

% Su, X., Tsai, C.-L., Wang, H., Nickerson, D. M., and Li, B. (2009), “Subgroup analysis via recursive partitioning,” Journal of Machine Learning Research, 10, 141–158.
% introduces interaction trees - applys to wage data
%
% Lipkovich, I., Dmitrienko, A., Denne, J., and Enas, G. (2011), “Subgroup identification based on differential effect search A recursive partitioning method for establishing response to treatment in patient subpopulations,” Statistics in Medicine, 30, 2601–2621.
% method of splitting to maximize treatment effect in one daughter node relative to other
% SIDES paper - has examples of personalized medicine in intro

% STIMA, others could also be included in this group

% subsubsection novel_algorithms_for_detecting_interactions (end)

%%%%%

% two-step methods: first step: estimate differential treatment effect of each ind by score function, then use scores as response in second step - (Cai et al., 2011; Zhao et al., 2013; Foster et al., 2011; Faries et al., 2013) \\
%   need to impose parametric models that may be misspecified, hard to interpret
%
% Cai, T., Tian, L., Wong, P. H., and Wei, L. (2011), “Analysis of randomized comparative clinical trial data for personalized treatment selections,” Biostatistics, 12, 270–282.
% talks about estrogen receptive cancer cells is that what I mean?
%
% Zhao, L., Tian, L., Cai, T., Claggett, B., and Wei, L.-J. (2013), “Effectively selecting a target population for a future comparative study,” Journal of the American Statistical Association, 108, 527–539.
%
% Foster, J. C., Taylor, J. M., and Ruberg, S. J. (2011), “Subgroup identification from randomized clinical trial data,” Statistics in Medicine, 30, 2867–2880.
%
% Faries, D. E., Chen, Y., Lipkovich, I., Zagar, A., Liu, X., and Obenchain, R. L. (2013), “Local control for identifying subgroups of interest in observational research: Persistence of treatment for major depressive disorder,” International Journal of Methods in Psychiatric Research, 22, 185–194.


%%%%%


% maximize value function
%
% Qian, M. and Murphy, S. A. (2011), “Performance guarantees for individualized treatment rules,” Annals of Statistics, 39, 1180.
%
% Zhao, Y., Zeng, D., Rush, A. J., and Kosorok, M. R. (2012), “Estimating individualized treatment rules using outcome weighted learning,” Journal of the American Statistical Association, 107, 1106–1118.
%
% Zhang, B., Tsiatis, A. A., Davidian, M., Zhang, M., and Laber, E. (2012), “Estimating optimal treatment regimes from a classification perspective,” Stat, 1, 103–114.

% All limited to single step?


% subsubsection statistical_methods_to_personalize_medicine (end)

\subsection{Dynamic treatment regimes} % (fold)
\label{sub:dynamic_treatment_regimes}

To formalize our idea of personalized medicine mentioned above, we can think of a personalized treatment as a decision rule, or a \emph{treatment regime} (or individualized treatment rule), that provides the best treatment given a patients' characteristics, or their \emph{state}.

Since for many diseases, patients often receive different stages of treatments over time, it is useful to generalize a single stage treatment regime into
a \emph{dynamic treatment regime}\footnote{ dynamic treatment regimes are also known as adaptive treatment strategies \textbf{[cite]} or treatment policies \textbf{cite}}, which is a generalization of the ideas of personalized medicine to sequencial decisions where the aspects of a person's state can vary over time. A dynamic treatment regime dictates treatments at each time point based on the changing (dynamic) states for every patient.

% subsection dynamic_treatment_regimes (end)

% How can reinforcement learning be of use in determining personalized treatments?



\subsection{overview of contents} % (fold)
\label{sub:overview_of_contents}

% subsection overview_of_contents (end)

\section{Reinforcement Learning} % (fold)
\label{sec:reinforcement_learning}

% similarities and differences from other types of learning}

Reinforcement learning is a branch of machine learning distinct from the other two main branches, supervised and unsupervised learning\footnote{As we will see however, techniques from both supervised and unsupervised learning can be useful in reinforcement learning problems.}. A main distinguishing feature of reinforcement learning is the emphasis on learning from experimentation, often over repeated stages.

\subsection{Basic process} % (fold)
\label{sub:basic_process}

The basic process of reinforcement learning involves a learning agent trying a sequence of actions, recording the consequences (rewards) of those actions, estimating the relationship between the actions and their consequences, and choosing a next action that leads to the best consequences (the action that maximizes the reward, so far as the agent can tell). This is where the ``reinforcement'' in reinforcement learning comes from: actions with favorable consequences (large rewards) tend to be repeated.

rewards

states, etc 

% subsection basic_process (end)



\subsection{Relationship to supervised and unsupervised learning} % (fold)
\label{sub:relationship_to_supervise_and_unsupervised_learning}

Supervised learning involves a learning agent being given a list of example actions to taken in given situations, with the goal being that the agent then extrapolates the correct example behavior to new situations. In reinforcement learning problems, agents do not receive direct instruction regarding which action they should take, instead they must learn which actions are best by trying them out. 

Unsupervised learning involves finding hidden structure in collections of example actions and situations where the correct action is unknown. This goal is distinct from the goal in reinforcement learning problems, which is to maximize a reward signal (though finding hidden structure may be useful to this end).

Unlike other machine learning problems, reinforcement learning problems are closed-loop: the actions of the agent affect the opportunities open to the agent later on. Further, the consequences of actions in reinforcement learning problems can manifest not only in the next opportunity, but in all subsequent opportunities.

% subsection relationship_to_supervise_and_unsupervised_learning (end)



% basic process of RL



% elements of RL problems?

% section reinforcement_learning (end)

\section{Q-learning} % (fold)
\label{sec:q_learning}

Q-learning is the most popular method for estimating dynamic treatment regimes.

% watkins

\subsection{Notation} % (fold)
\label{sub:notation}
Following the notation in Chakraborty [\textbf{CITE}], patients are recruited covariates measured before the trial begins that are recorded in a vector denoted $\bm{\bm{O}}_{1}$. These covariates will be referred to as the patient history at stage 1: $H_{1} \doteq \bm{O}_{1}$. A first treatment, $A_{1}$, is randomly assigned to each patient and each patient is measured again for the same covariates as before, $\bm{O}_{2}$ and scalar reward is received, $R_{1}$. This reward can be any scalar number for which maximizing would achieve the goals of the study \textbf{[S\&B]}. Most often rewards are simply set to be a continuous outcome of interest (e.g. number of kg of weight lost in a weight loss study). The treatment and second observation of covariates are incorporated in each patients history at stage 2: $H_{2} \doteq (\bm{O}_{1}, A_{1}, \bm{O}_{2})$. A second treatment, $A_{2}$ is then assigned to each patient at random (potentially depending on $Y_{1}$ and/or $H_{2}$) and the outcome is observed again, $Y_{2}$. Note that this can easily be modified for a single terminal outcome by setting $Y_{1} = 0$. The optimal Q-functions (Q for quality - quality of treatment decisions) are then defined as follows $\bm{r}_{k} = (R_{1k}, R_{2k}, \ldots, R_{nk})$


% subsection notation (end)

\subsection{Estimating Q-functions} % (fold)
\label{sub:estimating_q_functions}

Q-functions are defined as the expected rewards at each stage $k$ given that treatment(s) $\bm{A_{k}}$ was assigned and patient histories/characteristics up to stage $k$ were $\bm{H_{k}}$:

\begin{equation}
   Q_{k}(\bm{A}_{k}, \bm{H}_{k})  = \operatorname{E}[\bm{r}_{k} \mid \bm{A}_{k}, \bm{H}_{k}], \quad k = 1, \ldots, K
\end{equation}

Since Q-functions are conditional expectations, standard regression techniques are applicable and hence the most popular method for estimating Q-functions in the health sciences is ordinary least squares (linear regression) [\textbf{murphy DTR review}].
% \begin{equation}
%   Q_{j}^{opt}(H_{j}, A_{j}; \bm{\beta}_{j}, \bm{\psi}_{j}) = \bm{\beta}^{T} H_{j0} + \bm{\psi}^{T} H_{j1} A_{j}, \quad j = 1, 2
% \end{equation}
%
% Where $H_{j0}$ are main effects of history which includes an intercept term and a main effect for treatment, $A_{j}$
That being said, any modelling method can be used, and recently \textbf{Zhao et al} have used techniques from statistical learning, namely extremely randomized trees and support vector regression to fit Q-functions. Note that Q-functions may differ across stages of treatment.

\begin{enumerate}
  \item Set $Q^{opt}_{K + 1} \doteq 0$ (though any value will do)
  \item Create a pseudo-outcome for the last stage, $K$, estimation:
  \begin{align}
    \hat{\bm{r}}_{K} &= \bm{r}_{K} + \max_{a_{K+1}} Q_{K+1}^{opt}(\bm{A}_{K+1}, \bm{H}_{K+1}) \\
    \hat{\bm{r}}_{K} &= \bm{r}_{K}
  \end{align}
  \item Estimate the $K - 1$ (preceding stage) Q-function using the pseudo-outcome just created, $\hat{\bm{r}}_{K}$ as the outcome, all aspects of patient history at that stage, $\bm{H}_{K}$, desired as covariates, in addition to treatment by covariate interactions for covariates in $\bm{H}_{K}$ thought to be indicative of subgroups with differential treatment effects:
  \begin{equation}
      Q_{K}^{opt}(A_{2}, H_{2}) = \operatorname{E}[\hat{\bm{r}}_{K} \mid \bm{A}_{K}, \bm{H}_{K}]
  \end{equation}
  \item Create the pseudo-outcome for the previous stage:
  \begin{equation}
    \hat{\bm{r}}_{K-1} = \bm{r}_{K-1} + \max_{a_K} Q_{K}^{opt}(A_{K}, H_{K})
  \end{equation}
  \item Repeat until you've estimated the Q-function for the first stage
\end{enumerate}

The full recursive form is then 
  \begin{equation}
     Q_{k}(\bm{A}_{k}, \bm{H}_{k})  = \operatorname{E}[\bm{r}_{k} + \max_{a_{k+1}} Q_{k+1}(\bm{A}_{k+1}, \bm{H}_{k+1}) \mid \bm{A}_{k}, \bm{H}_{k}], \quad k = 1, \ldots, K
  \end{equation}

% backwards induction

\begin{algorithm}[H]
  initialize $Q_{K + 1}^{opt}$ arbitrarily \\
  $Q_{K + 1}^{opt} \gets 0$ \\
 \For{$k \in K, \ldots, 1$}{
  read current\;
  \eIf{understand}{
   go to next section\;
   current section becomes this one\;
   }{
   go back to the beginning of current section\;
  }
 }
 \caption{Q-learning algorithm}
\end{algorithm}

% subsubsection estimating_q_functions (end)

\subsection{Estimating optimal dynamic treatment regime} % (fold)
\label{sub:estimating_optimal_dynamic_treatment_regime}

The optimal treatment at each stage is then the treatment which maximizes the corresponding stage's Q-function:

\begin{equation}
  a^{opt}_{ik} = \argmax_{a_{ik}} Q_{k}(\bm{a}_{ik}, \bm{h}_{ik})
\end{equation}

For every participant $i = 1, \ldots, n$ at each stage $k = 1, \ldots, K$

% subsubsection estimating_optimal_dynamic_treatment_regime (end)

% subsection steps_in_two_stage_q_learning (end)



% section q_learning (end)

\section{Classification and Regression Trees (CART)} % (fold)
\label{sec:rpart}

Since in reinforcement learning rewards are defined to be continuous we consider only the regression tree version of the CART procedure.

A decision tree is a graphical representation of nested \texttt{if-then} statements, with each ``decision'' being based on whether the \texttt{if} statment is true or false. A regression tree is a kind of decision tree where each \texttt{if-then} statement splits the data (or subset of the data created by a previous split) according to the value of a single covariate at a time. Since an \texttt{if} statement creates two possible outcomes (\texttt{TRUE} or \texttt{FALSE}), each split is binary and results in two \emph{daughter nodes}, two subsets of the data created according to the value of a particular covariate. Once we've decided we've done enough splits, each \emph{terminal node} or \emph{leaf}, a node from which no further splits are made, is assigned an outcome (typically the mean outcome for each observation in that node).

For example, a simple regression tree could be constructed with the following rules:
\begin{verbatim}
if Covariate A <= 2 then
|   if Covariate B >= 34.2 then Outcome = 6
|   else Outcome = 4.9
else Outcome = 2.2
\end{verbatim}

Decision trees are depicted upside down compared to trees in nature:

<picture of example>

Example adapted from \cite{kuhn}

\subsection{Tree building} % (fold)
\label{sub:tree_building}

Our overarching goal in tree construction is to create nodes as homogeneous in the outcome as possible. This is because we will predict a single outcome for all observations in the terminal nodes, meaning nodes heterogeneous in the outcome will yield poor predictions. To achieve this we must determine

\begin{enumerate}
  \item What covariate to use to define a split and what value of the covariate to split on
  \item When to stop splitting
  \item How to assign an outcome to each terminal node
\end{enumerate}

We use the the classification and regression tree (CART) approach of \cite{CART}. We begin with all of the data (the \emph{root node}) and evaluate the sum of squared errors that result from splitting each unique value of each covariate:

\begin{equation}
  SSE = \sum_{i \in \text{node}_{L}} (y_{i} - \bar{y}_{L})^2 + \sum_{i \in \text{node}_{R}} (y_{i} - \bar{y}_{R})^2 
\end{equation} 

Where $\bar{y}_{L}$ is the average outcome in the left node, $\text{node}_{L}$ and $\bar{y}_{R}$ is the average outcome in the right node, $\text{node}_{R}$. We then choose the split that minimizes the sum of squares. Then the process is repeated for both of the resulting left and right nodes. This feature of recursive splitting, or partitioning, of the data is why this method is also known as \emph{recursive partitioning}. Typically we continue splitting in this manner until the number of observations in a node falls below a threshold (e.g. 20 observations).

\begin{algorithm}[H]
\While{stopping criteria not met}{
\For{each terminal node, $R_{m}$}{
 \For{each $X_{j} \in X$}{
  \For{each possible split of $X_{j}$}{
    compute change in SSE resulting from split
  }
 }
 \If{reduction in SSE from split largest}{
  split node
  }{
 }
 add newly created nodes to set of terminal nodes
 }
}
 \caption{CART Tree building algorithm}
\end{algorithm}

% subsection tree_building (end)

\subsection{Tree pruning} % (fold)
\label{sub:tree_pruning}

The large tree resulting from the tree building/growing phase, $T_{0}$, typically overfits the data. To combat this, we prune back $T_{0}$ using a technique called cost complexity pruning. 

The goal in cost-complexity pruning is to find, for each value of a complexity parameter $\alpha \geq 0$, the subtree $T \subset T_{0}$ created by collapsing any of $T_{0}$'s internal (non-terminal) nodes, which minimizes the cost complexity criterion:
%
\begin{equation}
    C_{\alpha}(T) = \sum_{m = 1}^{\abs{T}} N_{m} Q_{m}(T) + \alpha \abs{T}
\end{equation}
where
%
\begin{align}
  N_{m} &= \# \{x_{i} \in R_{m}\} \\
  \hat{c}_{m} &= \frac{1}{N_{m}} \sum_{x_{i} \in R_{m}} y_{i} \\
  Q_{m}(T) &= \frac{1}{N_{m}} \sum_{x_{i} \in R_{m}} (y_{i} - \hat{c}_{m})^2
\end{align}
and $\abs{T}$ is the number of terminal nodes in subtree $T$. One can show that there is a unique subtree $T_{\alpha}$ that minimizes $C_{\alpha}(T)$ for each given $\alpha$.

In order to find $T_{\alpha}$ for each $\alpha$, we use weakest link pruning. As the name implies, in weakest link pruning, we test out collapsing each internal node, choosing to actually collapse the one with that produces the smallest increase in $\sum_{m = 1}^{\abs{T}} N_{m} Q_{m}(T)$ per node (the weakest link). We continue in this fashion until we've returned to the root node. This sequence contains each $T_{\alpha}$. 

With the $T_{\alpha}$s in hand, we choose the $\alpha$ (and hence $T_{\alpha}$) that minimizes the sum of square errors produced using cross validation. Alternatively, we can use choose the largest $\alpha$ (smallest $T_{\alpha}$) which is within one standard error of the minimum cross validated error.

% subsubsection tree_pruning (end)

\subsection{Advantages and disadvantages to CART} % (fold)
\label{sub:advantages_and_disadvantages_to_cart}



% subsection advantages_and_disadvantages_to_cart (end)

% section rpart (end)

\section{Multivariate adaptive regression splines} % (fold)
\label{sec:mars}

Multivariate adaptive regression splines, or MARS (Friedman 1991) can be viewed as a generalization of stepwise selection regression methods that incorporates variable selection and automatic modelling of interactions. MARS accomplishes this through modelling covariates as piecewise linear basis functions and their products. These piecewise linear basis functions are of the form 
%
\begin{equation} \label{eq:bases}
  (x - t)_{+} \text{ and } (t - x)_{+}
\end{equation} where
%
\begin{equation}
  (Z)_{+} = \max(Z, 0) = \begin{cases}
  Z, & \text{if } Z > 0 \\
  0, & \text{otherwise}
  \end{cases}
\end{equation}
The set in equation \ref{eq:bases} sometimes called a \emph{reflected pair} because the values they take on are symmetric (mirrored) across $t$. Each function in the pair is called a \emph{hinge} function, and can alternatively be denoted $h(x - t)$ and $h(t - x)$ respectively. $t$ is called a $knot$ (where the two hinges join).

\subsection{Forward pass} % (fold)
\label{sub:forward_pass}

As in forward stepwise linear regression methods we begin with only a constant:
\begin{equation}
  f(X) = \beta_{0}
\end{equation}
Then, we consider adding a pair of piecewise linear functions made by splitting some covariate $X_{j}$ at a knot placed at one of the observed values of $X_{j}$, $x_{ij}$:
\begin{equation}
\beta_{1} (X_{j} - t)_{+} + \beta_{2}(t - X_{j})_{+}, \ i = 1, 2, \ldots, n; \ j = 1, 2, \ldots, p.
\end{equation}
To determine which predictor split at which knot to add, each of the $j$ predictors, split at each observed value $x_{ij}$ is evaluated in turn by adding each pair to the model and calculating the training error (e.g. the root-mean-square error--RMSE). The covariate and split point that cause the greatest reduction in error are added to the model.

Henceforth we consider the addition of a pair piecewise linear functions with general form
\begin{equation} \label{eq:gen-pairs-add}
  \beta_{M + 1} h_{\ell}(X) \cdot (X_{j} - t)_{+} + \beta_{M + 2} h_{\ell}(X) \cdot (t - X_{j})_{+}, \ h_{\ell}(X) \in \mathcal{M}, \ t \in \{x_{ij}\}.
\end{equation}
Where $\mathcal{M}$ is the set of hinge functions already in the model. Note that this equation also holds for the first step, in which case $h_{\ell}(X) = h_{0}(X) = 1$. As before, we add the pair that causes the greatest reduction in training error. To simplify the set of models considered, we may provide limit the degree, or order of interactions we are interested in considering. For example with degree = 1, we only consider an additive model (all $h_{\ell}(X) = h_{0}(X) = 1$), with degree = 2, we consider a maximum of two-way interactions, and so on.
Pairs are added until a user-defined limit on the number of terms in the model is reached, or the reduction in training error is smaller than some predefined threshold.

The resulting MARS regression function is of the form
\begin{equation}
  f(X) = \beta_{0} + \sum_{m = 1}^{M} \beta_{m} h_{m}(X),
\end{equation}
where $M$ is the number of terms in the model, and $h_{m}(X)$ is a hinge function like one of those in equation \ref{eq:bases} or a product of such functions.

% subsection forward_pass (end)

\subsection{Backward pass} % (fold)
\label{sub:backward_pass}

Usually, the function produced by the forward procedure is too large and overfits the data, so we use a pruning (backward deletion) procedure. For each term $h_{m}(X)$ in the model, we estimate how much the error would increase by removing it. Once we've removed one term, we repeat the procedure and delete another term. At each stage of this procedure we obtain $\hat{f}_{\lambda}(X)$, the best model of size $\lambda$ (best model with $\lambda$ terms). Note that we do not proceed backwards along the path through which the covariates were added (indeed we add pairs of hinge functions on at a time, but remove them one by one). 

To determine the optimal number of terms, $\lambda$, we can use a resampling technique (e.g. cross validation) but for computationally efficiency, we use generalized cross validation (GCV).

% subsection backward_pass (end)

\subsection{Generalized cross validation} % (fold)
\label{sub:generalized_cross_validation}

Generalized cross validation is a computational shortcut for linear regression models, which produces an error that approximates the leave-one-out cross-validated error. For MARS, the generalized cross-validation criterion is defined as
\begin{equation}
  GCV(\lambda) = 
    \frac{
      \sum_{i = 1}^{N}(y_{i} - \hat{f}_{\lambda}(x_{i}))^2
    }{
      (1 - M(\lambda)/N)^2
    }
\end{equation}
where $M(\lambda)$ is the effective number of parameters in the model, accounting for both the actual parameters and parameters used in selecting knot positions. Simulations and mathematical results tell us that $M(\lambda)$ should be set to $r + 3K$, the number of terms plus three times the number of knots, but can be set to $r + 2K$ when the model is restricted to be additive (degree = 1).

% subsection generalized_cross_validation (end)

\subsection{Relationship to CART} % (fold)
\label{sub:relationship_to_cart}

MARS can also be viewed as a modification of CART to improve performance in the regression setting. If we modified the MARS procedure to consider reflected pairs of step functions of the form $I(x - t > 0)$ and $I(t - x > 0)$ instead of the piecewise linear basis functions and replaced terms involved in interactions by the interaction term (making the term unavailable for further interactions), then the forward pass in MARS is equivalent to the CART tree-growing algorithm. In this case, the multiplication of a step function by a pair of reflected step functions is equivalent to splitting a node.

% subsection relationship_to_cart (end)

\subsection{Advantages of MARS} % (fold)
\label{sub:advantages_of_mars}

MARS has many of the same advantages as CART, in that it requires very litte pre-processing of data, it performs variable selection and models interactions as a part of the model building process, and the models produced are quite interpretable. This last point holds even for models with interactions since products of hinge functions are only nonzero when both hinge functions involved in the product are nonzero, allowing the term to operate only over a relatively small subspace of the two covariates. 

% subsection advantages_of_mars (end)

% section mars (end)

\section{Random forest} % (fold)
\label{sec:random_forest}

\subsection{Bagging} % (fold)
\label{sub:bagging}

As mentioned above, single trees tend to have quite high variance (small changes in the data can produce very different trees), however when grown sufficiently deep or bushy, they can have relatively low bias. Bootstrap aggregating (bagging for short) is a general technique that is useful whenever a modelling technique has these properties (high variance, low bias). 

Consider a collection of independent and identically distributed random variables, $X_{1}, X_{2}, \ldots, X_{n}$, with variance $\sigma^2$ and sample mean $\bar{X}$. It's easy to show that 
\begin{equation}
  \operatorname{Var}[\bar{X}] = \frac{\sigma^2}{n},
\end{equation}
the variance of the sample mean lower by a factor of $n$ than the variance of any single observation. If we could build trees on many different samples then average the results, we could exploit this property to drastically reduce the variance and therefore increase performance.

Typically, however, we only have one sample to work with so we can take $B$ bootstrap samples of our one sample to approximate these samples, then aggregate the results into a single prediction. In the case of regression problems like ours, this typically amounts to averaging the predictions for each observation and choosing that as the predicted outcome. 

\begin{algorithm}[H]
\For{$i$ in 1 to $B$}{
  Take a bootstrap sample of the data \\
  Fit model to this sample (e.g. grow an unpruned tree)
 }
 \caption{Bagging}
\end{algorithm}

In fact, this method provides a built-in way to estimate the test error by only averaging predictions for any given tree using observations that were not in the bootstrap sample used to train that tree (called the out-of-bag sample, about 1/3 of the observations on average). This out-of-bag sample typically approximates the cross validation estimate of error rate quite well, but requires much less computation.

As you might have guessed, we can't in practice achieve a $B$ fold decrease in the variance from a single tree because our trees are not independent. This is most egregious when there is a single very strong predictor. This will tend to be the first few splits, and thus strongly influence the remaining shape of the tree, and lead to highly correlated trees across bootstrap samples. 

% subsection bagging (end)

\subsection{Random forest} % (fold)
\label{sub:random_forest}

Statistically, we can decrease correlation by introducing randomness, so \cite{rf} suggested that instead of considering all $p$ predictors for each split, to only consider a random subset $m_{try} \subset p$, randomly sampled from $p$ for each split. 

\begin{algorithm}[H]
\For{$i$ in 1 to $B$}{
  take a bootstrap sample of the data \\
  Grow a tree on this sample: \\
  \While{stopping criteria not met}{
  \For{each split}{
    randomly select $m_{try}$ of $p$ original covariates \\
    split on a covariate in $m_{try}$ causing largest decrease in SSE
   }
  }
  
  (do not prune)
 }
 \caption{Random forest}
\end{algorithm}

How should we choose $B$ and $m_{try}$? There is no danger to overfitting by growing more trees so $B$ is typically set at a large enough number for the error rate to level off, \cite{khun} recommends starting at 1000. Recommendations for $m_{try}$ are often set at a default of $\sqrt{p}$ or $p/3$, the latter recommended for regression problems, though $m_{try}$ can be also be chosen using resampling techniques. 

% subsection random_forest (end)

\subsection{Advantages and disadvantages to random forest} % (fold)
\label{sub:advantages_and_disadvantages_to_random_forest}

This approach leads to significantly more accurate predictions over a single tree, but also significantly reduces the interpretability.

% subsection advantages_and_disadvantages_to_random_forest (end)

variable importance in rf

% section random_forest (end)

\section{Simulation}

\subsection{Patient Model} % (fold)
\label{sub:vpm}

Largely inspired by \cite{paper}

The goals of the patient model were to capture:

\begin{enumerate}
  \item treatment kills tumor cells but increases toxicity (decreases quality of life)
  \item growth of tumor in absence of treatment
  \item reduction of toxicity in absence of treatment
  \item Better quality of life or general health reduces the growth rate of tumor cells (and vice versa)
  \item Tumors decrease quality of life, and larger tumors cause larger decreases
\end{enumerate} 

\subsubsection{Transition functions} % (fold)
\label{ssub:transition_functions}

Change in patient quality of life was modelled as a function of tumor mass and dosage of treatment, with doses above 0.5 decreasing quality of life and doses below 0.5 allowing quality of life to increase (toxicity to decrease) due to fewer toxic effects from treatment:

\begin{equation}
W_{t*} = 0.1 M_{t-1} + 1.2 (D_{t-1} - 0.5) + W_{t - 1}
\end{equation}
\begin{equation}
W_{t} = \begin{cases}
  W_{t*} &\text{if } W_{t*} > 0 \\
  0 &\text{if } W_{t*} \leq 0
\end{cases}
\end{equation}

Where $W_{t}$ is the negative of the patient quality of life (toxicity) at month $t$, which is bounded at 0 (no toxic effects from treatment),
$M_{t-1}$ is the tumor mass the previous month,
$D_{t-1}$ is the dose of treatment for the previous month, and
$W_{t}$ is the negative of the patient quality of life (toxicity) at the previous month.

Tumor mass change was modelled as a function of patient health and dosage, with doses above 0.5 decreasing tumor mass and doses below 0.5 causing tumor mass to increase:

\begin{equation}
M_{t*} = [0.15 W_{t-1} - 1.2 (D_{t-1} - 0.5) + M_{t - 1}] I(M_{t-1} > 0)
\end{equation}
\begin{equation}
M_{t} = \begin{cases}
  M_{t*} &\text{if } M_{t*} > 0 \\
  0 &\text{if } M_{t*} \leq 0
\end{cases}
\end{equation}

Where $M_{t}$ is the tumor mass at month $t$, which is bounded at 0 since mass is strictly positive,
$W_{t-1}$ is patient toxicity at month $t - 1$,
$M_{t-1}$ is the tumor mass at month $t - 1$,
$D_{t-1}$ is the dose assigned at month $t - 1$, and
$I(M_{t-1} > 0)$ indicates that if a person is cured (previous tumor mass = 0), then tumors do not recur.

% subsubsection transition_functions (end)

\subsubsection{Survival model} % (fold)
\label{ssub:survival_model}

Survival times (in months) were generated from an exponential distribution with a rate parameter modeled as a function of the tumor mass and toxicity at time $t$:
\begin{equation}
  \lambda_{t}(M_{t+1}, W_{t+1}) = \exp(-7 + W_{t+1} + 1.2 M_{t+1} + W_{t+1} M_{t+1}).
\end{equation}
This formulation indicates that we take tumor mass to be more important in survival than toxicity. The interaction between tumor mass and toxicity reflects the belief that higher levels of both tumor mass and toxicity lead to a greater reduction in survival than either alone.

% subsubsection survival_model (end)

% subsection vpm (end)


\subsubsection{rewards} % (fold)
\label{sub:}

Rewards were defined as the log of the observed survival time.

% subsection  (end)

\subsection{Basic scenario} % (fold)
\label{sub:basic_setup}

To generate training data, I began by simulating 1000 patients, each receiving at total of six months of treatment ($t = 0, 1, \ldots, 5$), the doses of which were chosen from a uniform distribution over 0 and 1 at the beginning of each month:
\begin{equation}
  D_{t} \overset{iid}{\sim} \text{Unif}(0, 1), \quad t = 0, 1, \ldots, 5
\end{equation}

Each patient's initial tumor mass, $M_{0}$ and negative of quality of life (toxicity), $W_{0}$ were generated independently from uniform distributions between 0 and 2:

\begin{equation}
  W_{0} \overset{iid}{\sim} \text{Unif}(0, 2), \qquad
  M_{0} \overset{iid}{\sim} \text{Unif}(0, 2)
\end{equation}

At each month $t$, the next month's tumor mass, $M_{t + 1}$, and toxicity, $W_{t + 1}$, were computed in response to dose at month $t$, $D_{t}$, for each patient as described above. The rate parameter for the survival distribution, $\lambda_{t+1}(M_{t + 1}, W_{t + 1})$, was updated for each patient from these next month's tumor masses and toxicities. Survival times, $S_{t}$ were then randomly sampled from the exponential distribution with the just computed rate parameter:
\begin{equation}
  S_{t} \sim \text{Exp}(\exp(-7 + W_{t+1} + 1.2 M_{t+1} + W_{t+1} M_{t+1}))
\end{equation}

If $S_{t} \leq 1$ (i.e. the patient would not survive past another month), the patient was said to have died before the next month, and the reward received was computed as the log of the total number of months the patient had survived (including the fraction of a month before their death). At the final month $t = 5$, the rewards were computed as $\log(S_{5} + 5)$, the log of the simulated survival time after the final treatment plus the the number of months already elapsed. 
\begin{equation}
  R_{t + 1} = \begin{cases}
    \log(S_{t} + \sum_{i = 0}^{t} i) & \text{if } S_{t} \leq 1 \text{ or } t = 5 \\
    0 & \text{otherwise}
  \end{cases}
\end{equation}

\subsubsection{Tuning parameters for basic scenario} % (fold)
\label{ssub:tuning_parameters_for_basic_scenario}

\begin{itemize}
  \item CART: the complexity parameter $\alpha$ was chosen using 10 fold cross validation as ????
  \item MARS: the number of terms in each model, $\lambda$ was chosen using generalized cross validation
  \item RF: the number of parameters to consider for splitting in each resample, \texttt{mtry} was fixed at 2 plus the treatment variable \texttt{dose} which was chosen in each resample, making this equivalent to bagging
\end{itemize}

% subsubsection tuning_parameters_for_basic_scenario (end)

% subsection basic_setup (end)


\subsection{Subgroups/Interaction scenario} % (fold)
\label{sub:subgroups_interaction}

In addition to the basic scenario just described, two additional baseline covariates, $X_{1}$ and $X_{2}$ were simulated from uniform distributions over 0, 1:
\begin{equation}
  X_{1}, X_{2} \overset{iid}{\sim} \text{Unif}(0, 1) 
\end{equation}
Using the values of these covariates, four approximately equally sized subgroups were constructed:
\begin{enumerate}
  \item $X_{1} < 0.5$ \& $X_{2} < 0.5$: patients simulated identically to those in the basic scenario.
  \item $X_{1} > 0.5$ \& $X_{2} < 0.5$: these patients are 50\% more sensitive to the effects of the drug:
   \begin{equation}
   W'_{t*} = 0.1 M_{t-1} + 1.2 (1.5 D_{t-1} - 0.5) + W_{t - 1}
   \end{equation}
   \item $X_{1} < 0.5$ \& $X_{2} > 0.5$: the treatment is 50\% more effective for these patients:
\begin{equation}
M'_{t*} = [0.15 W_{t-1} - 1.2 (1.5 D_{t-1} - 0.5) + M_{t - 1}] I(M_{t-1} > 0)
\end{equation}
   \item $X_{1} < 0.5$ \& $X_{2} > 0.5$: the drug is 50\% more effective for these patients, but they are also 50\% more sensitive to its effects:
   \begin{align}
   W'_{t*} &= 0.1 M_{t-1} + 1.2 (1.5 D_{t-1} - 0.5) + W_{t - 1} \\
   M'_{t*} &= [0.15 W_{t-1} - 1.2 (1.5 D_{t-1} - 0.5) + M_{t - 1}] I(M_{t-1} > 0)
   \end{align}
\end{enumerate}

\subsubsection{Tuning parameters for interaction scenario} % (fold)
\label{ssub:tuning_parameters_for_interaction_scenario}

\begin{itemize}
  \item CART: the complexity parameter $\alpha$ was chosen using 10 fold cross validation as ????
  \item MARS: the number of terms in each model, $\lambda$ was chosen using generalized cross validation
  \item RF: the number of parameters to consider for splitting in each resample, \texttt{mtry} was fixed at 2 plus the treatment variable \texttt{dose} which was chosen in each resample
\end{itemize}

% subsubsection tuning_parameters_for_interaction_scenario (end)

% subsection subgroups_interaction (end)

\subsection{High-dimensional noise scenarios} % (fold)
\label{sub:noise_variables}

The basic scenario was also repeated in the presence of 100 noise variables, $Z = (Z_{1}, \ldots, Z_{10})$ and $V = (V_{1}, \ldots, V_{90})$ with distributions as follows:
\begin{align}
  Z_{1}, \ldots, Z_{5} &\overset{iid}{\sim} N(1, 1) \\
  Z_{6}, \ldots, Z_{10} &\overset{iid}{\sim} N(-1, 1) \\
  V_{1}, \ldots, V_{90} &\overset{iid}{\sim} N(0, 1)
\end{align}
That is, $Z_{1}, \ldots, Z_{5}$ were sampled independently from a normal distribution with mean 1 and standard deviation 1, $Z_{6}, \ldots, Z_{10}$ were sampled independently from a normal distribution with mean -1 and standard deviation 1, and $V_{1}, \ldots, V_{90}$ were sampled independently from a standard normal distribution. These covariates were used in two separate scenarios described below.

\subsubsection{Pure noise scenario} % (fold)
\label{ssub:pure_noise}

In the first noise scenario, pure noise, the above variables were made available to the models, but had no effect on $W_{t}, M_{t}$, or $S_{t}$.

% subsubsection pure_noise (end)

\subsubsection{Predictive noise scenario} % (fold)
\label{ssub:predictive_noise}
In the second noise scenario, predictive noise, the $Z$ variables had a small influence on the rate parameter in the survival distribution:
\begin{equation}
  \lambda'_{t}(M_{t+1}, W_{t+1}) = \exp\del{-7 + W_{t+1} + 1.2 M_{t+1} + W_{t+1} M_{t+1} + 0.05 \sum_{i = 1}^{10} Z_{i}}.
\end{equation}
but did not change the optimal dose, as this modification only shifts the expected survival up or down and does not change the dose chosen to achieve the maximum survival over the range of possible doses.

% subsubsection predictive_noise (end)

% subsubsection noise_variables (end)

\subsubsection{Both interaction and noise scenario} % (fold)
\label{ssub:subsubsection_name}

The interaction and noise scenarios were also combined together, to create the same subgroups as defined above in the presence of pure noise, and the presence of predictive noise.

% subsubsection ssubsubsection_name (end)

\subsubsection{Tuning parameters for all noise scenarios} % (fold)
\label{ssub:tuning_parameters_for_all_noise_scenarios}

\begin{itemize}
  \item CART: the complexity parameter $\alpha$ was chosen using 10 fold cross validation as ????
  \item MARS: the number of terms in each model, $\lambda$ was chosen using generalized cross validation
  \item RF: the number of parameters to consider for splitting in each resample, \texttt{mtry} was fixed at 10 ($\approx \sqrt{p}$), plus the treatment variable \texttt{dose} which was chosen in each resample
\end{itemize}

% subsubsection tuning_parameters_for_all_noise_scenarios (end)


% \begin{equation}
% \hat{Q}_{t} = R_{t} + \max_{a_{t + 1}} \hat{Q}_{t+1}(S_{t+1}, a_{t+1})
% \end{equation}

\subsection{Procedure} % (fold)
\label{sub:procedure}

With the training data generated from each scenario listed above, Q-learning was applied using each of CART, MARS, and random forest to estimate the Q-functions at each stage. The fitted models for each stage under each scenario were saved for use on the validation set. 

% subsection procedure (end)

\subsection{Validation} % (fold)
\label{sub:validation}

For validation data, I simulated the starting conditions for 200 in the same way as for the training data. I made 15 copies of these individuals which would undergo one of the following regimes:

\begin{enumerate}
  \item 10 constant dose regimes (doses of $0.1, 0.2, \ldots,$ or 1 at each stage)
  \item ``One-on-one-off'' - an alternating constant dose regime starting at the maximum dose, switching dose to 0 for next stage, and repeating through the end of the trial
  \item ``Best'' - the best greedy dose choice given complete knowledge of the transition functions and their influence on survival
  \item The estimated best doses (ones that maximize expected survival) from each of the three methods to estimate Q-functions: CART, MARS, and random forest
\end{enumerate}

At each stage, patients would receive the treatment as dictated by the regimes above.


% subsection validation (end)

% section simulation (end)

\section{Simulation Results} % (fold)
\label{sec:simulation_results}

% section simulation_results (end)

\section{Discussion} % (fold)
\label{sec:discussion}

% section discussion (end)

\end{document}

% \begin{equation}
% R_{t} = R_{t, 1} + R_{t, 2} + R_{t, 3}
% \end{equation}
%
% First, a large negative reward of -60 was given if the patient died, since a primary endpoint is typically overall survival:
%
% \begin{equation}
% R_{t, 1}(M_{t-1}, H_{t-1}, D_{t-1}) =
% \begin{cases}
%   -60 & \text{patient died} \\
%   0 & \text{otherwise}
% \end{cases}
% \end{equation}
%
% Second, a small negative reward of -5 was given for an decrease in patient health above 0.5 and a small positive reward of 5 if patient health increased by more than 0.5:
%
% \begin{equation}
% R_{t, 2}(M_{t-1}, H_{t-1}, D_{t-1}) =
% \begin{cases}
%   5 & \dot{H} > 0.5 \\
%   0 & -0.5 \leq \dot{H} \leq 0.5 \\
%   -5 & \dot{H} < -0.5
% \end{cases}
% \end{equation}
%
% Third, similar to the rewards for toxicity, a small negative reward of -5 was given for an increase in tumor mass above 0.5, no reward if the tumor mass did not increase or decrease by more than 0.5, a small positive reward of 5 if the tumor mass decreased by more than 0.5, and a moderate positive reward of 15 was given if the patient was cured (the resulting tumor mass was less than or equal to zero):
%
% \begin{equation}
% R_{t, 3}(M_{t-1}, H_{t-1}, D_{t-1}) =
% \begin{cases}
%   -5 & \dot{M} > 0.5 \\
%   0 & -0.5 \leq \dot{M} \leq 0.5 \\
%   5 & \dot{M} < -0.5 \\
%   15 & \dot{M} + M \leq 0
% \end{cases}
% \end{equation}
%
% 
%
% \subsection{death} % (fold)
% \label{sub:death}
%
% \begin{equation}
% \lambda(t) = -7 + H_{t} + M_{t}
% \end{equation}
%
% \begin{equation}
% \Delta F(t) = e^{-\lambda}
% \end{equation}
%
% \begin{equation}
% p = 1 - \Delta F
% \end{equation}
%
% \begin{equation}
% \text{patient died} \sim B(p)
% \end{equation}
%
% % subsection death (end)
