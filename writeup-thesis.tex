\documentclass[12pt]{article}
\usepackage[margin = 1in]{geometry}

% makes table of contents and such hyperlinks
\usepackage{hyperref}

% nicely formatted tables
\usepackage{booktabs}

% H to force figures and tables to where they appear in text
\usepackage{float}

% English language/hyphenation
\usepackage[english]{babel}

% > < print corectly, accented words hyphenate
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% allows usage of \enquote to quote things
\usepackage{csquotes}

% \usepackage[backend = bibtex, sorting=none, hyperref = true]{biblatex}
% \addbibresource{spray.bib}
\usepackage{cite}
\usepackage{ctable}

\usepackage{amsmath}
\usepackage{commath}

%--------------------------------------------------------------------------
%  TITLE SECTION
%--------------------------------------------------------------------------

\title{\normalfont \Large Title}

\author{\normalsize \sl Kyle Humphrey}

\date{\normalsize \sl \today}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Why should you care about personalized medicine} % (fold)
\label{sub:why_should_you_care_about_personalized_medicine}

\subsubsection{What reinforcement learning has to do with personalized medicine} % (fold)
\label{ssub:what_reinforcement_learning_has_to_do_with_personalized_medicine}

% subsubsection what_reinforcement_learning_has_to_do_with_personalized_medicine (end)

% subsection why_should_you_care_about_personalized_medicine (end) 

\section{Reinforcement Learning} % (fold)
\label{sec:reinforcement_learning}

% similarities and differences from other types of learning}

Reinforcement learning is a branch of machine learning distinct from the other two main branches, supervised and unsupervised learning. As we will see however, techniques from both supervised and unsupervised learning can be useful in reinforcement learning problems.

Supervised learning involves a learning agent being given a list of example actions to taken in given situations, with the goal being that the agent then extrapolates the correct example behavior to new situations. In reinforcement learning problems, agents do not receive direct instruction regarding which action they should take, instead they must learn which actions are best by trying them out. 

Unsupervised learning involves finding hidden structure in collections of example actions and situations where the correct action is unknown. The goal in reinforcement learning problems is to maximize a reward signal (though finding hidden structure may be useful to this end).

Unlike other machine learning problems, reinforcement learning problems are closed-loop: the actions of the agent affect the opportunities open to the agent later on. Further, the consequences of actions in reinforcement learning problems can manifest not only in the next opportunity, but in all subsequent opportunities.

% basic process of RL

The basic process of reinforcement learning involves a learning agent trying a sequence of actions, recording the consequences (rewards) of those actions, estimating the relationship between the actions and their consequences, and choosing a next action that leads to the best consequences (the action that maximizes the reward, so far as the agent can tell). This is where the ``reinforcement'' in reinforcement learning comes from: actions with favorable consequences (large rewards) tend to be repeated.

% elements of RL problems?

\subsubsection{policy, bellman equation, value functions, ?TD learning, ?eligibility traces} % (fold)
\label{ssub:policy_bellman_equation_value_functions_td_learning_eligibility_traces}

% subsubsection policy_bellman_equation_value_functions_td_learning_eligibility_traces (end)

% section reinforcement_learning (end)

\section{Q-learning} % (fold)
\label{sec:q_learning}

How it fits in with the above

Properties

single stage defintion

\[
Q(s_{t}, a{t}) \leftarrow Q(s_{t}, a{t}) + \alpha_{t} \del{r_{t+1} + \gamma \max_{a} Q(s_{s + 1}, a)}
\]

recursive form



% section q_learning (end)

\section{MARS} % (fold)
\label{sec:mars}

% section mars (end)

\section{other modelling methods} % (fold)
\label{sec:other_modelling_methods}

% section other_modelling_methods (end)

\section{Simulation setup}

\subsection{Virtual Patient Model} % (fold)
\label{sub:vpm}

I began by simulating 1000 patients, each receiving six months of treatment. Each patient had a random starting tumor mass and patient health, both generated from uniform distributions between 0 and 2. The initial treatment dose was selected from a uniform distribution between 0.5 and 1 (so that everyone entered the trial receiving some treatment). Subsequent doses were selected from a uniform distribution between 0 and 1 at the beginning of each month.

Patient health change was modelled as a function of tumor mass and dosage of treatment, with doses above 0.5 decreasing health and doses below 0.5 allowing health to increase due to less toxic effects from the treatment.

\[
\dot{H}_{t} = - 0.1 M_{t-1} - 1.2 (D_{t-1} - 0.5)
\]

Where $\dot{H}_{t}$ is the change in patient health from month $t-1$ to month $t$, 
$M_{t-1}$ is the tumor mass the previous month, and 
$D_{t-1}$ is the dose of treatment for the previous month

Tumor mass change was modelled as a function of patient health and dosage, with doses above 0.5 decreasing tumor mass and doses below 0.5 allowing it to increase:

\[
\dot{M}_{t} = [- 0.15 H_{t-1} - 1.2 (D_{t-1} - 0.5)] I_{\{M_{t-1} > 0\}}(M_{t-1})
\]

Where $\dot{M}_{t}$ is the change in tumor mass from one month to another,
$H_{t-1}$ is the health from the previous month,
$M_{t-1}$ is the tumor mass the previous month,
$D_{t-1}$ is the dose for the previous month,
$I_{\{M_{t-1} > 0\}}(M)$ indicates that if a person is cured (previous tumor mass = 0), then tumors do not recur.

% subsection vpm (end)


\subsection{} % (fold)
\label{sub:}

The reward was defined as the sum of three parts: 

\[
R_{t} = R_{t, 1} + R_{t, 2} + R_{t, 3}
\]

First, a large negative reward of -60 was given if the patient died, since a primary endpoint is typically overall survival:

\[
R_{t, 1}(M_{t-1}, H_{t-1}, D_{t-1}) = 
\begin{cases}
  -60 & \text{patient died} \\
  0 & \text{otherwise}
\end{cases}
\]

Second, a small negative reward of -5 was given for an decrease in patient health above 0.5 and a small positive reward of 5 if patient health increased by more than 0.5:

\[
R_{t, 2}(M_{t-1}, H_{t-1}, D_{t-1}) =  
\begin{cases}
  5 & \dot{H} > 0.5 \\
  0 & -0.5 \leq \dot{H} \leq 0.5 \\
  -5 & \dot{H} < -0.5
\end{cases}
\]

Third, similar to the rewards for toxicity, a small negative reward of -5 was given for an increase in tumor mass above 0.5, no reward if the tumor mass did not increase or decrease by more than 0.5, a small positive reward of 5 if the tumor mass decreased by more than 0.5, and a moderate positive reward of 15 was given if the patient was cured (the resulting tumor mass was less than or equal to zero):

\[
R_{t, 3}(M_{t-1}, H_{t-1}, D_{t-1}) = 
\begin{cases}
  -5 & \dot{M} > 0.5 \\
  0 & -0.5 \leq \dot{M} \leq 0.5 \\
  5 & \dot{M} < -0.5 \\
  15 & \dot{M} + M \leq 0
\end{cases}
\]

% subsection  (end)

\subsection{death} % (fold)
\label{sub:death}

\[
\lambda(t) = -7 + H_{t} + M_{t}
\]

\[
\Delta F(t) = e^{-\lambda}
\]

\[
p = 1 - \Delta F
\]

\[
\text{patient died} \sim B(p)
\]

% subsection death (end)


\[
\hat{Q}_{t} = R_{t} + max_{a_{t + 1}} \hat{Q}_{t+1}(S_{t+1}, a_{t+1})
\]

\section{simulation} % (fold)
\label{sec:simulation}



% section simulation (end)









\end{document}
