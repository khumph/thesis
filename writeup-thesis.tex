\documentclass[12pt]{article}
\usepackage{geometry}

% makes table of contents and such hyperlinks
\usepackage{hyperref}

% nicely formatted tables
\usepackage{booktabs}

% H to force figures and tables to where they appear in text
\usepackage{float}

% bold math symbols (including greek)
\usepackage{bm}

% English language/hyphenation
\usepackage[english]{babel}

% > < print corectly, accented words hyphenate
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% allows usage of \enquote to quote things
\usepackage{csquotes}

% \usepackage[backend = bibtex, sorting=none, hyperref = true]{biblatex}
% \addbibresource{spray.bib}
\usepackage{cite}
\usepackage{ctable}

\usepackage{amsmath}
\usepackage{commath}

\DeclareMathOperator*{\argmax}{argmax}

%--------------------------------------------------------------------------
%  TITLE SECTION
%--------------------------------------------------------------------------

\title{\normalfont \Large Thesis Title}

\author{\normalsize \sl Kyle Humphrey}

\date{\normalsize \sl \today}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Personalized medicine} % (fold)
\label{sub:personalized_medicine}

% Why personalized medicine?}

Patients often show significant differences in their responses to treatment. [\textbf{provide some examples}] Significant gains in public health can be made from identifying subgroups and the corresponding treatments which are most effective for them, rather than relying on identification of only the most effective treatment overall.

% What is personalized medicine?

This is the idea of \emph{personalized medicine}\footnote{Personalized medicine is also called precision medicine, stratified medicine, and P4 medicine}, which is the commonsensical notion that the best treatment for a given patient depends upon that patients' characteristics \cite{defn-paper}. The term is most often used today to when the characteristics are biomarkers, perhaps whether metastatic breast cancer cells  overexpress human epidermal growth factor receptor 2 (HER2), and these biomarkers are used to determine the best treatment for a particular individual, and when to use it. In the case of the patients with HER2-positive cells, they should receive a different treatment (trastuzumab) from those with HER2-negative cells \cite{} .
% When/where did personalized medicine start?
However, the general idea goes back at least as far as Hippocrates \textbf{[cite]}, and are implemented in principle whenever a physician tries to inform a treatment decision by determining whether the cause of an infection is bacterial or viral.

% subsection personalized_medicine (end) 


% What are some challenges to implementing/discovering personalized treatments?}

Lots of possible subgroups, complexity of disease

% What are some methods that have been used in the past to find personalized treatment regimes?
\subsection{Statistical methods to personalize medicine} % (fold)
\label{sub:statistical_methods_to_personalize_medicine}

% subsection statistical_methods_to_personalize_medicine (end)

\subsubsection{Traditional methods} % (fold)
\label{ssub:traditional_methods}

% traditional methods: ad hoc subgroup analyses, or searching for treatment by subgroups interactions 

Statistically, the personalization of medicine involves the identification of patient subgroups which respond to treatment differently. Traditionally, this is accomplished by investigation of (often ad hoc) treatment by covariate interactions, where values of the covariate identify subgroups. Using the HER2 breast cancer example above, we could encapsulate our knowledge of a patient's HER2 status into a binary covariate which indicates whether she was HER2 positive or negative, then statistically test if the coefficient on an interaction term between the indicator of HER2 status and treatment is significantly different from 0.

% Problems: Curse of dimensionality, multiple comparisons

While straightforward, this approach has several problems. First, which subgroups to examine is often a subjective judgement made by the researcher (and where to cutoff a group that is formed based on a continuous covariate). Second, since the numer of plausible subgroups can grow large quickly the number of interaction terms required can grow quite large in typical problems causing a reduction in power and requiring an much greater growth of sample size (curse of dimensionality). There are also issues to be addressed with multiple comparisons.

\subsubsection{Algorithms for detecting interactions} % (fold)
\label{ssub:algorithms_for_detecting_interactions}

In response to the above problem with the traditional approach, several novel algorithms to detect interactions have been developed.

interaction trees - easy to interpret, final trees don't connect with any objective function - hard to determine optimal treatment for patients

% Su, X., Tsai, C.-L., Wang, H., Nickerson, D. M., and Li, B. (2009), “Subgroup analysis via recursive partitioning,” Journal of Machine Learning Research, 10, 141–158.
% introduces interaction trees - applys to wage data
%
% Lipkovich, I., Dmitrienko, A., Denne, J., and Enas, G. (2011), “Subgroup identification based on differential effect search A recursive partitioning method for establishing response to treatment in patient subpopulations,” Statistics in Medicine, 30, 2601–2621.
% method of splitting to maximize treatment effect in one daughter node relative to other
% SIDES paper - has examples of personalized medicine in intro

% STIMA, others could also be included in this group

% subsubsection novel_algorithms_for_detecting_interactions (end)

%%%%%

% two-step methods: first step: estimate differential treatment effect of each ind by score function, then use scores as response in second step - (Cai et al., 2011; Zhao et al., 2013; Foster et al., 2011; Faries et al., 2013) \\
%   need to impose parametric models that may be misspecified, hard to interpret
%
% Cai, T., Tian, L., Wong, P. H., and Wei, L. (2011), “Analysis of randomized comparative clinical trial data for personalized treatment selections,” Biostatistics, 12, 270–282.
% talks about estrogen receptive cancer cells is that what I mean?
%
% Zhao, L., Tian, L., Cai, T., Claggett, B., and Wei, L.-J. (2013), “Effectively selecting a target population for a future comparative study,” Journal of the American Statistical Association, 108, 527–539.
%
% Foster, J. C., Taylor, J. M., and Ruberg, S. J. (2011), “Subgroup identification from randomized clinical trial data,” Statistics in Medicine, 30, 2867–2880.
%
% Faries, D. E., Chen, Y., Lipkovich, I., Zagar, A., Liu, X., and Obenchain, R. L. (2013), “Local control for identifying subgroups of interest in observational research: Persistence of treatment for major depressive disorder,” International Journal of Methods in Psychiatric Research, 22, 185–194.


%%%%%


% maximize value function
%
% Qian, M. and Murphy, S. A. (2011), “Performance guarantees for individualized treatment rules,” Annals of Statistics, 39, 1180.
%
% Zhao, Y., Zeng, D., Rush, A. J., and Kosorok, M. R. (2012), “Estimating individualized treatment rules using outcome weighted learning,” Journal of the American Statistical Association, 107, 1106–1118.
%
% Zhang, B., Tsiatis, A. A., Davidian, M., Zhang, M., and Laber, E. (2012), “Estimating optimal treatment regimes from a classification perspective,” Stat, 1, 103–114.

% All limited to single step?


% subsubsection statistical_methods_to_personalize_medicine (end)

\subsection{Dynamic treatment regimes} % (fold)
\label{sub:dynamic_treatment_regimes}

To formalize our idea of personalized medicine mentioned above, we can think of a personalized treatment as a decision rule, or a \emph{treatment regime} (or individualized treatment rule), that provides the best treatment given a patients' characteristics, or their \emph{state}.

Since for many diseases, patients often receive different stages of treatments over time, it is useful to generalize a single stage treatment regime into
a \emph{dynamic treatment regime}\footnote{ dynamic treatment regimes are also known as adaptive treatment strategies \textbf{[cite]} or treatment policies \textbf{cite}}, which is a generalization of the ideas of personalized medicine to sequencial decisions where the aspects of a person's state can vary over time. A dynamic treatment regime dictates treatments at each time point based on the changing (dynamic) states for every patient.

% subsection dynamic_treatment_regimes (end)

% How can reinforcement learning be of use in determining personalized treatments?



\subsection{overview of contents} % (fold)
\label{sub:overview_of_contents}

% subsection overview_of_contents (end)

\section{Reinforcement Learning} % (fold)
\label{sec:reinforcement_learning}

% similarities and differences from other types of learning}

Reinforcement learning is a branch of machine learning distinct from the other two main branches, supervised and unsupervised learning. As we will see however, techniques from both supervised and unsupervised learning can be useful in reinforcement learning problems.

Supervised learning involves a learning agent being given a list of example actions to taken in given situations, with the goal being that the agent then extrapolates the correct example behavior to new situations. In reinforcement learning problems, agents do not receive direct instruction regarding which action they should take, instead they must learn which actions are best by trying them out. 

Unsupervised learning involves finding hidden structure in collections of example actions and situations where the correct action is unknown. The goal in reinforcement learning problems is to maximize a reward signal (though finding hidden structure may be useful to this end).

Unlike other machine learning problems, reinforcement learning problems are closed-loop: the actions of the agent affect the opportunities open to the agent later on. Further, the consequences of actions in reinforcement learning problems can manifest not only in the next opportunity, but in all subsequent opportunities.

% basic process of RL

The basic process of reinforcement learning involves a learning agent trying a sequence of actions, recording the consequences (rewards) of those actions, estimating the relationship between the actions and their consequences, and choosing a next action that leads to the best consequences (the action that maximizes the reward, so far as the agent can tell). This is where the ``reinforcement'' in reinforcement learning comes from: actions with favorable consequences (large rewards) tend to be repeated.

% elements of RL problems?

% section reinforcement_learning (end)

\section{Q-learning} % (fold)
\label{sec:q_learning}

Q-learning is the most popular method for estimating dynamic treatment regimes.

% watkins

\subsection{Notation} % (fold)
\label{sub:notation}
Following the notation in Chakraborty [\textbf{CITE}], patients are recruited covariates measured before the trial begins that are recorded in a vector denoted $\bm{\bm{O}}_{1}$. These covariates will be referred to as the patient history at stage 1: $H_{1} \doteq \bm{O}_{1}$. A first treatment, $A_{1}$, is randomly assigned to each patient and each patient is measured again for the same covariates as before, $\bm{O}_{2}$ and scalar reward is received, $R_{1}$. This reward can be any scalar number for which maximizing would achieve the goals of the study \textbf{[S\&B]}. Most often rewards are simply set to be a continuous outcome of interest (e.g. number of kg of weight lost in a weight loss study). The treatment and second observation of covariates are incorporated in each patients history at stage 2: $H_{2} \doteq (\bm{O}_{1}, A_{1}, \bm{O}_{2})$. A second treatment, $A_{2}$ is then assigned to each patient at random (potentially depending on $Y_{1}$ and/or $H_{2}$) and the outcome is observed again, $Y_{2}$. Note that this can easily be modified for a single terminal outcome by setting $Y_{1} = 0$. The optimal Q-functions (Q for quality - quality of treatment decisions) are then defined as follows $\bm{r}_{k} = (R_{1k}, R_{2k}, \ldots, R_{nk})$


% subsection notation (end)

\subsection{Estimating Q-functions} % (fold)
\label{sub:estimating_q_functions}

Q-functions are defined as the expected rewards at each stage $k$ given that treatment(s) $\bm{A_{k}}$ was assigned and patient histories/characteristics up to stage $k$ were $\bm{H_{k}}$:

\begin{equation}
   Q_{k}(\bm{A}_{k}, \bm{H}_{k})  = \operatorname{E}[\bm{r}_{k} \mid \bm{A}_{k}, \bm{H}_{k}], \quad k = 1, \ldots, K
\end{equation}

Since Q-functions are conditional expectations, standard regression techniques are applicable and hence the most popular method for estimating Q-functions in the health sciences is ordinary least squares (linear regression) [\textbf{murphy DTR review}].
% \begin{equation}
%   Q_{j}^{opt}(H_{j}, A_{j}; \bm{\beta}_{j}, \bm{\psi}_{j}) = \bm{\beta}^{T} H_{j0} + \bm{\psi}^{T} H_{j1} A_{j}, \quad j = 1, 2
% \end{equation}
%
% Where $H_{j0}$ are main effects of history which includes an intercept term and a main effect for treatment, $A_{j}$
That being said, any modelling method can be used, and recently \textbf{Zhao et al} have used techniques from statistical learning, namely extremely randomized trees and support vector regression to fit Q-functions. Note that Q-functions may differ across stages of treatment.

\begin{enumerate}
  \item Set $Q^{opt}_{K + 1} \doteq 0$ (though any value will do)
  \item Create a pseudo-outcome for the last stage, $K$, estimation:
  \begin{align}
    \hat{\bm{r}}_{K} &= \bm{r}_{K} + \max_{a_{K+1}} Q_{K+1}^{opt}(\bm{A}_{K+1}, \bm{H}_{K+1}) \\
    \hat{\bm{r}}_{K} &= \bm{r}_{K}
  \end{align}
  \item Estimate the $K - 1$ (preceding stage) Q-function using the pseudo-outcome just created, $\hat{\bm{r}}_{K}$ as the outcome, all aspects of patient history at that stage, $\bm{H}_{K}$, desired as covariates, in addition to treatment by covariate interactions for covariates in $\bm{H}_{K}$ thought to be indicative of subgroups with differential treatment effects:
  \begin{equation}
      Q_{K}^{opt}(A_{2}, H_{2}) = \operatorname{E}[\hat{\bm{r}}_{K} \mid \bm{A}_{K}, \bm{H}_{K}]
  \end{equation}
  \item Create the pseudo-outcome for the previous stage:
  \begin{equation}
    \hat{\bm{r}}_{K-1} = \bm{r}_{K-1} + \max_{a_K} Q_{K}^{opt}(A_{K}, H_{K})
  \end{equation}
  \item Repeat until you've estimated the Q-function for the first stage
\end{enumerate}

The full recursive form is then 
  \begin{equation}
     Q_{k}(\bm{A}_{k}, \bm{H}_{k})  = \operatorname{E}[\bm{r}_{k} + \max_{a_{k+1}} Q_{k+1}(\bm{A}_{k+1}, \bm{H}_{k+1}) \mid \bm{A}_{k}, \bm{H}_{k}], \quad k = 1, \ldots, K
  \end{equation}

% backwards induction

% subsubsection estimating_q_functions (end)

\subsection{Estimating optimal dynamic treatment regime} % (fold)
\label{sub:estimating_optimal_dynamic_treatment_regime}

The optimal treatment at each stage is then the treatment which maximizes the corresponding stage's Q-function:

\[
  a^{opt}_{ik} = \argmax_{a_{ik}} Q_{k}(\bm{a}_{ik}, \bm{h}_{ik})
\]

For every participant $i = 1, \ldots, n$ at each stage $k = 1, \ldots, K$

% subsubsection estimating_optimal_dynamic_treatment_regime (end)

% subsection steps_in_two_stage_q_learning (end)



% section q_learning (end)

\section{rpart} % (fold)
\label{sec:rpart}

% section rpart (end)

\section{MARS} % (fold)
\label{sec:mars}

% section mars (end)

\section{random forest} % (fold)
\label{sec:random_forest}

% section random_forest (end)

\section{boosted trees} % (fold)
\label{sec:boosted_trees}

% section boosted_trees (end)

\section{other modelling methods} % (fold)
\label{sec:other_modelling_methods}

% section other_modelling_methods (end)

\section{Simulation}

\subsection{Patient Model} % (fold)
\label{sub:vpm}

To generate training data, I began by simulating 1000 patients, each receiving six months of treatment. Each patient had a random starting tumor mass and patient health, both generated from uniform distributions between 0 and 2. The initial treatment dose was selected from a uniform distribution between 0.5 and 1 (so that everyone entered the trial receiving some treatment). Subsequent doses were selected from a uniform distribution between 0 and 1 at the beginning of each month.

Patient health change was modelled as a function of tumor mass and dosage of treatment, with doses above 0.5 decreasing health and doses below 0.5 allowing health to increase due to less toxic effects from the treatment.

\[
\Delta W = - 0.1 M_{t-1} - 1.2 (D_{t-1} - 0.5)
\]

Where $\dot{H}_{t}$ is the change in patient health from month $t-1$ to month $t$, 
$M_{t-1}$ is the tumor mass the previous month, and 
$D_{t-1}$ is the dose of treatment for the previous month

Tumor mass change was modelled as a function of patient health and dosage, with doses above 0.5 decreasing tumor mass and doses below 0.5 allowing it to increase:

\[
\dot{M}_{t} = [- 0.15 H_{t-1} - 1.2 (D_{t-1} - 0.5)] I_{\{M_{t-1} > 0\}}(M_{t-1})
\]

Where $\dot{M}_{t}$ is the change in tumor mass from one month to another,
$H_{t-1}$ is the health from the previous month,
$M_{t-1}$ is the tumor mass the previous month,
$D_{t-1}$ is the dose for the previous month,
$I_{\{M_{t-1} > 0\}}(M)$ indicates that if a person is cured (previous tumor mass = 0), then tumors do not recur.

% subsection vpm (end)


\subsection{rewards} % (fold)
\label{sub:}

The reward was defined as the sum of three parts: 

\[
R_{t} = R_{t, 1} + R_{t, 2} + R_{t, 3}
\]

First, a large negative reward of -60 was given if the patient died, since a primary endpoint is typically overall survival:

\[
R_{t, 1}(M_{t-1}, H_{t-1}, D_{t-1}) = 
\begin{cases}
  -60 & \text{patient died} \\
  0 & \text{otherwise}
\end{cases}
\]

Second, a small negative reward of -5 was given for an decrease in patient health above 0.5 and a small positive reward of 5 if patient health increased by more than 0.5:

\[
R_{t, 2}(M_{t-1}, H_{t-1}, D_{t-1}) =  
\begin{cases}
  5 & \dot{H} > 0.5 \\
  0 & -0.5 \leq \dot{H} \leq 0.5 \\
  -5 & \dot{H} < -0.5
\end{cases}
\]

Third, similar to the rewards for toxicity, a small negative reward of -5 was given for an increase in tumor mass above 0.5, no reward if the tumor mass did not increase or decrease by more than 0.5, a small positive reward of 5 if the tumor mass decreased by more than 0.5, and a moderate positive reward of 15 was given if the patient was cured (the resulting tumor mass was less than or equal to zero):

\[
R_{t, 3}(M_{t-1}, H_{t-1}, D_{t-1}) = 
\begin{cases}
  -5 & \dot{M} > 0.5 \\
  0 & -0.5 \leq \dot{M} \leq 0.5 \\
  5 & \dot{M} < -0.5 \\
  15 & \dot{M} + M \leq 0
\end{cases}
\]

% subsection  (end)

\subsection{death} % (fold)
\label{sub:death}

\[
\lambda(t) = -7 + H_{t} + M_{t}
\]

\[
\Delta F(t) = e^{-\lambda}
\]

\[
p = 1 - \Delta F
\]

\[
\text{patient died} \sim B(p)
\]

% subsection death (end)



\section{simulation} % (fold)
\label{sec:simulation}

\[
\hat{Q}_{t} = R_{t} + \max_{a_{t + 1}} \hat{Q}_{t+1}(S_{t+1}, a_{t+1})
\]



% section simulation (end)









\end{document}
