\documentclass[12pt]{article}
\usepackage[margin = 1in]{geometry}

% makes table of contents and such hyperlinks
\usepackage{hyperref}

% nicely formatted tables
\usepackage{booktabs}

% H to force figures and tables to where they appear in text
\usepackage{float}

% English language/hyphenation
\usepackage[english]{babel}

% > < print corectly, accented words hyphenate
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% allows usage of \enquote to quote things
\usepackage{csquotes}

% \usepackage[backend = bibtex, sorting=none, hyperref = true]{biblatex}
% \addbibresource{spray.bib}
\usepackage{cite}
\usepackage{ctable}

\usepackage{amsmath}
\usepackage{commath}

%--------------------------------------------------------------------------
%  TITLE SECTION
%--------------------------------------------------------------------------

\title{\normalfont \Large Title}

\author{\normalsize \sl Kyle Humphrey}

\date{\normalsize \sl \today}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Why should you care about personalized medicine} % (fold)
\label{sub:why_should_you_care_about_personalized_medicine}


% Why personalized medicine?}

Patients often show significant differences in their responses to treatment. [\textbf{provide some examples}] Significant gains in public health can be made from identifying subgroups and the corresponding treatments which are most effective for them, rather than relying on identification of only the most effective treatment overall.

% What is personalized medicine?
This is the idea of personalized medicine (also called precision medicine, stratified medicine, or P4 medicine), which is the commonsensical notion that the best treatment for a given patient depends upon that patients' characteristics \textbf{[cite]}. The term is most often used today to describe a process where biomarkers, perhaps whether breast cancer cells are HER2-positive, are used to determine the best treatment for a particular individual, and when to use it. \textbf{[defn-paper]}.
% When/where did personalized medicine start?
However, the general ideas go back at least as far as Hippocrates \textbf{[cite]}, and are implemented in principle whenever a physician tries to inform a treatment decision by determining whether the cause of an infection is bacterial or viral.

% What are some methods that have been used in the past to find personalized treatment regimes?


:
\begin{itemize}
  \item traditional methods: ad hoc subgroup analyses, or searching for treatment by subgroups interactions \\
    \textbf{Problem}: Curse of dimensionality, multiple comparisons
  \item novel algorithms for detecting covariate-treatment interactions: interaction trees \\
  easy to interpret, final trees don't connect with any objective function - hard to determine optimal treatment for patients
  
  Su, X., Tsai, C.-L., Wang, H., Nickerson, D. M., and Li, B. (2009), “Subgroup analysis via recursive partitioning,” Journal of Machine Learning Research, 10, 141–158.
  
  Lipkovich, I., Dmitrienko, A., Denne, J., and Enas, G. (2011), “Subgroup identification based on differential effect searchA recursive partitioning method for establishing response to treatment in patient subpopulations,” Statistics in Medicine, 30, 2601–2621.
  
  \item two-step methods: first step: estimate differential treatment effect of each ind by score function, then use scores as response in second step - (Cai et al., 2011; Zhao et al., 2013; Foster et al., 2011; Faries et al., 2013) \\
  need to impose parametric models that may be misspecified, hard to interpret
  
  Cai, T., Tian, L., Wong, P. H., and Wei, L. (2011), “Analysis of randomized comparative clinical trial data for personalized treatment selections,” Biostatistics, 12, 270–282.
  
  Zhao, L., Tian, L., Cai, T., Claggett, B., and Wei, L.-J. (2013), “Effectively selecting a target population for a future comparative study,” Journal of the American Statistical Association, 108, 527–539.
  
  Foster, J. C., Taylor, J. M., and Ruberg, S. J. (2011), “Subgroup identification from randomized clinical trial data,” Statistics in Medicine, 30, 2867–2880.
  
  Faries, D. E., Chen, Y., Lipkovich, I., Zagar, A., Liu, X., and Obenchain, R. L. (2013), “Local control for identifying subgroups of interest in observational research: Persistence of treatment for major depressive disorder,” International Journal of Methods in Psychiatric Research, 22, 185–194.
  
  \item maximize value function
  
  Qian, M. and Murphy, S. A. (2011), “Performance guarantees for individualized treatment rules,” Annals of Statistics, 39, 1180.
  
  Zhao, Y., Zeng, D., Rush, A. J., and Kosorok, M. R. (2012), “Estimating individualized treatment rules using outcome weighted learning,” Journal of the American Statistical Association, 107, 1106–1118.
  
  Zhang, B., Tsiatis, A. A., Davidian, M., Zhang, M., and Laber, E. (2012), “Estimating optimal treatment regimes from a classification perspective,” Stat, 1, 103–114.
\end{itemize}

All limited to single step?

% What are some challenges to implementing/discovering personalized treatments?}

Lots of possible subgroups, complexity of disease

% Dynamic Treatment regimes

To formalize our idea of personalized medicine mentioned above, we can think of a personalized treatment as a decision rule, or a \emph{treatment regime} (or individualize treatment rule), that provides the best treatment given a patients' characteristics, or their \emph{state}.

Since for many diseases, patients often receive different stages of treatments over time, it is useful to generalize a single stage treatment regime into
a \emph{dynamic treatment regime}, which is a generalization of the ideas of personalized medicine to sequencial decisions where the aspects of a person's state can vary over time. A dynamic treatment regime dictates treatments at each time point based on the changing (dynamic) states for every patient (dynamic treatment regimes are also known as adaptive treatment strategies \textbf{[cite]} or treatment policies \textbf{cite}).

% How can reinforcement learning be of use in determining personalized treatments?

% subsection why_should_you_care_about_personalized_medicine (end) 

\subsection{overview of contents} % (fold)
\label{sub:overview_of_contents_of_thesis}

% subsection overview_of_contents_of_thesis (end)

\section{Reinforcement Learning} % (fold)
\label{sec:reinforcement_learning}

% similarities and differences from other types of learning}

Reinforcement learning is a branch of machine learning distinct from the other two main branches, supervised and unsupervised learning. As we will see however, techniques from both supervised and unsupervised learning can be useful in reinforcement learning problems.

Supervised learning involves a learning agent being given a list of example actions to taken in given situations, with the goal being that the agent then extrapolates the correct example behavior to new situations. In reinforcement learning problems, agents do not receive direct instruction regarding which action they should take, instead they must learn which actions are best by trying them out. 

Unsupervised learning involves finding hidden structure in collections of example actions and situations where the correct action is unknown. The goal in reinforcement learning problems is to maximize a reward signal (though finding hidden structure may be useful to this end).

Unlike other machine learning problems, reinforcement learning problems are closed-loop: the actions of the agent affect the opportunities open to the agent later on. Further, the consequences of actions in reinforcement learning problems can manifest not only in the next opportunity, but in all subsequent opportunities.

% basic process of RL

The basic process of reinforcement learning involves a learning agent trying a sequence of actions, recording the consequences (rewards) of those actions, estimating the relationship between the actions and their consequences, and choosing a next action that leads to the best consequences (the action that maximizes the reward, so far as the agent can tell). This is where the ``reinforcement'' in reinforcement learning comes from: actions with favorable consequences (large rewards) tend to be repeated.

% elements of RL problems?

\subsubsection{policy, bellman equation, value functions, ?TD learning, ?eligibility traces} % (fold)
\label{ssub:policy_bellman_equation_value_functions_td_learning_eligibility_traces}

% subsubsection policy_bellman_equation_value_functions_td_learning_eligibility_traces (end)

% section reinforcement_learning (end)

\section{Q-learning} % (fold)
\label{sec:q_learning}

Q-learning is a model-free method, which means it doesn't use an explicit model of the environment or planning, to estimate optimal action-selection policies. The maximized Q-function is the same as the optimal value function:

learned action-value function $Q$ directly approximates $q_{*}$ the optimal action value function

\[
  V^{*}_{t} = \max_{a_{t}} Q(s_{t}, a_{t})
\]

$Q(s_{T}, a)$ for the terminal state $T$ is set at an arbitrary fixed value:

\[
  Q(s_{T}, a) \gets 0
\]

The Q-function for the previous state is then updated:

\[
Q(s_{T-1}, a_{T-1}) \gets Q(s_{T-1}, a_{T-1}) + \alpha_{T-1} \del{r_{T} + \gamma  \max_{a_{T}} Q(s_{T}, a_{T}) - Q(s_{T-1}, a_{T-1})}
\]

Where $\gamma$ is a discount factor $\in (0, 1]$, $\alpha_{T-1}$ is the learning rate or step size parameter $\in (0, 1]$, $r_{t+1}$ is the observed reward received after performing action $a_{T}$ in $s_{T}$. 

value iteration/backwards induction

How it fits in with the above

Properties

single stage defintion

\[
Q(s_{t}, a{t}) \gets Q(s_{t}, a{t}) + \alpha_{t} \del{r_{t+1} + \gamma V^{*}_{t + 1}}
\]

\[
V^{*}_{t} = \max_{a_{t}} Q(s_{t}, a_{t})
\]

\[
Q(s_{t}, a{t}) \gets Q(s_{t}, a_{t}) + \alpha_{t} \del{r_{t+1} + \gamma \max_{a_{t}} Q(s_{s + 1}, a_{t})}
\]

recursive form



% section q_learning (end)

\section{MARS} % (fold)
\label{sec:mars}

% section mars (end)

\section{other modelling methods} % (fold)
\label{sec:other_modelling_methods}

% section other_modelling_methods (end)

\section{Simulation}

\subsection{Patient Model} % (fold)
\label{sub:vpm}

To generate training data, I began by simulating 1000 patients, each receiving six months of treatment. Each patient had a random starting tumor mass and patient health, both generated from uniform distributions between 0 and 2. The initial treatment dose was selected from a uniform distribution between 0.5 and 1 (so that everyone entered the trial receiving some treatment). Subsequent doses were selected from a uniform distribution between 0 and 1 at the beginning of each month.

Patient health change was modelled as a function of tumor mass and dosage of treatment, with doses above 0.5 decreasing health and doses below 0.5 allowing health to increase due to less toxic effects from the treatment.

\[
\Delta W = - 0.1 M_{t-1} - 1.2 (D_{t-1} - 0.5)
\]

Where $\dot{H}_{t}$ is the change in patient health from month $t-1$ to month $t$, 
$M_{t-1}$ is the tumor mass the previous month, and 
$D_{t-1}$ is the dose of treatment for the previous month

Tumor mass change was modelled as a function of patient health and dosage, with doses above 0.5 decreasing tumor mass and doses below 0.5 allowing it to increase:

\[
\dot{M}_{t} = [- 0.15 H_{t-1} - 1.2 (D_{t-1} - 0.5)] I_{\{M_{t-1} > 0\}}(M_{t-1})
\]

Where $\dot{M}_{t}$ is the change in tumor mass from one month to another,
$H_{t-1}$ is the health from the previous month,
$M_{t-1}$ is the tumor mass the previous month,
$D_{t-1}$ is the dose for the previous month,
$I_{\{M_{t-1} > 0\}}(M)$ indicates that if a person is cured (previous tumor mass = 0), then tumors do not recur.

% subsection vpm (end)


\subsection{rewards} % (fold)
\label{sub:}

The reward was defined as the sum of three parts: 

\[
R_{t} = R_{t, 1} + R_{t, 2} + R_{t, 3}
\]

First, a large negative reward of -60 was given if the patient died, since a primary endpoint is typically overall survival:

\[
R_{t, 1}(M_{t-1}, H_{t-1}, D_{t-1}) = 
\begin{cases}
  -60 & \text{patient died} \\
  0 & \text{otherwise}
\end{cases}
\]

Second, a small negative reward of -5 was given for an decrease in patient health above 0.5 and a small positive reward of 5 if patient health increased by more than 0.5:

\[
R_{t, 2}(M_{t-1}, H_{t-1}, D_{t-1}) =  
\begin{cases}
  5 & \dot{H} > 0.5 \\
  0 & -0.5 \leq \dot{H} \leq 0.5 \\
  -5 & \dot{H} < -0.5
\end{cases}
\]

Third, similar to the rewards for toxicity, a small negative reward of -5 was given for an increase in tumor mass above 0.5, no reward if the tumor mass did not increase or decrease by more than 0.5, a small positive reward of 5 if the tumor mass decreased by more than 0.5, and a moderate positive reward of 15 was given if the patient was cured (the resulting tumor mass was less than or equal to zero):

\[
R_{t, 3}(M_{t-1}, H_{t-1}, D_{t-1}) = 
\begin{cases}
  -5 & \dot{M} > 0.5 \\
  0 & -0.5 \leq \dot{M} \leq 0.5 \\
  5 & \dot{M} < -0.5 \\
  15 & \dot{M} + M \leq 0
\end{cases}
\]

% subsection  (end)

\subsection{death} % (fold)
\label{sub:death}

\[
\lambda(t) = -7 + H_{t} + M_{t}
\]

\[
\Delta F(t) = e^{-\lambda}
\]

\[
p = 1 - \Delta F
\]

\[
\text{patient died} \sim B(p)
\]

% subsection death (end)



\section{simulation} % (fold)
\label{sec:simulation}

\[
\hat{Q}_{t} = R_{t} + \max_{a_{t + 1}} \hat{Q}_{t+1}(S_{t+1}, a_{t+1})
\]



% section simulation (end)









\end{document}
